{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import IterativeImputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6c997a350e4d0b"
  },
  {
   "cell_type": "markdown",
   "id": "df780251-bb74-4a3f-aa64-2526d6edb9fd",
   "metadata": {},
   "source": [
    "#### Step 0: Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c9c938-2c74-485c-893d-e62b42fb7a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_file_path(base_path, site_name):\n",
    "    input_path = os.path.join(base_path, site_name) + '/'\n",
    "    output_path = os.path.join(base_path, site_name, 'processed_data') + '/'\n",
    "    aux_path = os.path.join(base_path, 'aux_files') + '/'\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    return [input_path, output_path, aux_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d696b011-3605-4f9a-bf0b-7b05846a273f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_onset_data(file_paths):\n",
    "    xxx = pd.read_pickle(file_paths[0] + 'AKI_LAB_SCR'+'.pkl')\n",
    "    yyy = pd.read_pickle(file_paths[0] + 'AKI_ONSETS'+'.pkl') \n",
    "    yyy = yyy[['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'DISCHARGE_DATE']]\n",
    "    xxx = xxx[['ENCOUNTERID', 'PATID', 'SPECIMEN_DATE',  'RESULT_NUM']] \n",
    "    xxx = xxx.merge(yyy, on = ['ENCOUNTERID', 'PATID'], how='left')\n",
    "    xxx = xxx.dropna()\n",
    "    xxx['DAYS_SINCE_ADMIT'] = (xxx['SPECIMEN_DATE']-xxx['ADMIT_DATE']).dt.days\n",
    "    # take daily average\n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'DAYS_SINCE_ADMIT', 'RESULT_NUM', 'ADMIT_DATE']].groupby(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'DAYS_SINCE_ADMIT', 'ADMIT_DATE']).mean()\n",
    "    xxx = xxx.sort_values(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'])\n",
    "    xxx = xxx.reset_index()\n",
    "    return xxx, yyy.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c65fc-c6ae-43ca-af1d-1a2831922c65",
   "metadata": {},
   "source": [
    "#### Step 1: Get AKI Onset Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43759942-8abf-4e1c-a589-f3b0e842e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define auxiliary functions for baseline caculation\n",
    "KDIGO_baseline = np.array([\n",
    "    [1.5, 1.3, 1.2, 1.0],\n",
    "    [1.5, 1.2, 1.1, 1.0],\n",
    "    [1.4, 1.2, 1.1, 0.9],\n",
    "    [1.3, 1.1, 1.0, 0.9],\n",
    "    [1.3, 1.1, 1.0, 0.8],\n",
    "    [1.2, 1.0, 0.9, 0.8]\n",
    "])\n",
    "KDIGO_baseline = pd.DataFrame(KDIGO_baseline, columns = [\"Black males\", \"Other males\",\n",
    "                                                        \"Black females\", \"Other females\"],\n",
    "                             index = [\"20-24\", \"25-29\", \"30-39\", \"40-54\", \"55-65\", \">65\"])\n",
    "\n",
    "def inverse_MDRD(row, KDIGO_baseline):\n",
    "    age = row[\"AGE\"]\n",
    "    is_male = True if row[\"MALE\"]  else False\n",
    "    is_black = True if row[\"RACE_BLACK\"] else False\n",
    "        \n",
    "    if is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black males\"]\n",
    "    \n",
    "    if is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other males\"]\n",
    "\n",
    "    if not is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black females\"]\n",
    "    \n",
    "    if not is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other females\"]\n",
    "        \n",
    "def inverse_MDRD_raw(row):\n",
    "    eGFR = row['DFLT_eGFR']  \n",
    "    male = row['MALE']\n",
    "    black = row['RACE_BLACK']\n",
    "    age = row['AGE']\n",
    "    \n",
    "    if male:\n",
    "        gender_factor = 1.0\n",
    "    else:\n",
    "        gender_factor = 0.742\n",
    "\n",
    "    if black:\n",
    "        race_factor = 1.212\n",
    "    else:\n",
    "        race_factor = 1.0\n",
    "\n",
    "    Scr = (eGFR / (175 * (age ** -0.203) * gender_factor * race_factor)) ** (1 / -1.154)\n",
    "    return Scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f964fffc-3979-4c26-bb02-97f18e985d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scr_baseline_new(df_scr, df_admit, file_paths,  aggfunc_7d = 'last', aggfunc_1y = 'mean', keep_ckd = False):\n",
    "    cohort_table = dict()\n",
    "    \n",
    "    # load & process dx data\n",
    "    dx = pd.read_pickle(file_paths[0]+'AKI_DX.pkl') \n",
    "    \n",
    "    existing_columns = [col for col in ['PATID', 'ENCOUNTERID', 'DX', 'DX_DATE', 'DX_TYPE', 'DAYS_SINCE_ADMIT']\n",
    "                        if col in dx.columns]\n",
    "    dx = dx[existing_columns]\n",
    "    dx = df_admit[['PATID', 'ENCOUNTERID', 'ADMIT_DATE']].merge(dx, on = ['PATID', 'ENCOUNTERID'], how = 'inner')\n",
    "\n",
    "    if 'DAYS_SINCE_ADMIT' not in dx.columns:\n",
    "        dx['DAYS_SINCE_ADMIT'] = (dx['DX_DATE']-dx['ADMIT_DATE']).dt.days\n",
    "        \n",
    "    # calculate DX_DATE when it is missing\n",
    "    dx.loc[dx.DX_DATE.isna(), 'DX_DATE'] = \\\n",
    "            dx.loc[dx.DX_DATE.isna(), 'ADMIT_DATE'] + \\\n",
    "            pd.to_timedelta(dx.loc[dx.DX_DATE.isna(), 'DAYS_SINCE_ADMIT'], unit='D')  # Use admit date of the index encounter, not the dx recording encounter.\n",
    "\n",
    "    dx['DX'] = dx['DX'].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].replace('09', '9')\n",
    "    \n",
    "    # load & process demo data\n",
    "    demo = pd.read_pickle(file_paths[0]+'AKI_DEMO'+'.pkl')  \n",
    "    demo['MALE'] = demo['SEX'] == 'M'\n",
    "\n",
    "    demo['RACE_WHITE'] = demo['RACE'] == '05'\n",
    "    demo['RACE_BLACK'] = demo['RACE'] == '03'\n",
    "    demo = demo[['PATID', 'ENCOUNTERID', 'AGE', 'MALE', 'RACE_WHITE', 'RACE_BLACK']]\n",
    "    demo = demo.drop_duplicates()\n",
    "    \n",
    "    pat_id_cols = ['PATID', 'ENCOUNTERID']\n",
    "    complete_df = df_scr[['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    " \n",
    "    admission_SCr = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE) & \\\n",
    "                                (complete_df.SPECIMEN_DATE <= (complete_df.ADMIT_DATE + pd.Timedelta(days=1)))].copy()\n",
    "\n",
    "    admission_SCr = admission_SCr.groupby(pat_id_cols)['RESULT_NUM'].mean().reset_index()\n",
    "\n",
    "    admission_SCr.rename(columns = {'RESULT_NUM': 'ADMISSION_SCR'}, inplace = True)\n",
    "\n",
    "    complete_df = complete_df.merge(admission_SCr, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    one_week_prior_admission = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE - pd.Timedelta(days=7)) & \\\n",
    "                                           (complete_df.SPECIMEN_DATE < complete_df.ADMIT_DATE)].copy()\n",
    "    one_week_prior_admission = one_week_prior_admission.sort_values(by = ['PATID', 'ENCOUNTERID','SPECIMEN_DATE'])\n",
    "    \n",
    "    scr_1w_time = one_week_prior_admission.copy() #.groupby(pat_id_cols).last().reset_index()\n",
    "    scr_1w_time['days_before_admit']  = (scr_1w_time['ADMIT_DATE']  -  scr_1w_time['SPECIMEN_DATE']).dt.days\n",
    "    cohort_table['scr_1w_df'] = scr_1w_time\n",
    "    \n",
    "    one_week_prior_admission = one_week_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].agg(aggfunc_7d).reset_index()\n",
    "\n",
    "        \n",
    "    one_week_prior_admission = one_week_prior_admission.rename(columns = {'RESULT_NUM': 'ONE_WEEK_SCR'})\n",
    "\n",
    "    complete_df = complete_df.merge(one_week_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), 'BASELINE_EST_1'] = \\\n",
    "                np.nanmin(complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), ['ONE_WEEK_SCR','ADMISSION_SCR']], axis = 1)\n",
    "\n",
    "    complete_dfe = complete_df.drop(['SPECIMEN_DATE', 'RESULT_NUM'],axis=1).drop_duplicates()\n",
    "    cohort_table['ALL_ENCOUNTERS'] = len(complete_dfe[['PATID','ENCOUNTERID']].drop_duplicates())\n",
    "    cohort_table['ALL_PATIENTS'] = complete_dfe.PATID.nunique()\n",
    "    cohort_table['ADMISSION_SCR_YES'] = complete_dfe.ADMISSION_SCR.notna().sum()\n",
    "    cohort_table['ADMISSION_SCR_NO'] = complete_dfe.ADMISSION_SCR.isna().sum()\n",
    "    cohort_table['ONE_WEEK_SCR_YES'] = complete_dfe.ONE_WEEK_SCR.notna().sum()\n",
    "    cohort_table['ONE_WEEK_SCR_NO'] = complete_dfe.ONE_WEEK_SCR.isna().sum()    \n",
    "    cohort_table['ADMISSION_AND_1W_SCR'] = (complete_dfe.ADMISSION_SCR.notna() & complete_dfe.ONE_WEEK_SCR.notna()).sum()\n",
    "    cohort_table['ADMISSION_AND_1W_SCR_MIN'] = (complete_dfe.BASELINE_EST_1.notna()).sum()\n",
    "    cohort_table['ADMISSION_OR_1W_SCR'] = (complete_dfe.ADMISSION_SCR.notna() | complete_dfe.ONE_WEEK_SCR.notna()).sum()\n",
    "    cohort_table['ONE_WEEK_SCR_ENC'] = complete_dfe[(complete_dfe.ONE_WEEK_SCR.notna() & (complete_dfe['ONE_WEEK_SCR']==complete_dfe['BASELINE_EST_1']))]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ADMISSION_SCR_1W_ENC'] = complete_dfe[(complete_dfe.ONE_WEEK_SCR.notna() & (complete_dfe['ONE_WEEK_SCR']!=complete_dfe['BASELINE_EST_1']))]['ENCOUNTERID'].unique()\n",
    "        \n",
    "    #ori_num_unique_combinations = df_scr.groupby(['PATID', 'ENCOUNTERID']).ngroups\n",
    "    # criterion1_no_missing = complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), :].groupby(pat_id_cols).ngroups\n",
    "    # criterion1_missing_rate = 1 - criterion1_no_missing / ori_num_unique_combinations\n",
    "\n",
    "    one_year_prior_admission = complete_df[(complete_df.SPECIMEN_DATE < (complete_df.ADMIT_DATE - pd.Timedelta(days=7))) & \\\n",
    "                                     (complete_df.SPECIMEN_DATE >= (complete_df.ADMIT_DATE - pd.Timedelta(days=365.25)))].copy()\n",
    "    one_year_prior_admission = one_year_prior_admission.sort_values(by = ['PATID', 'ENCOUNTERID','SPECIMEN_DATE'])\n",
    "    \n",
    "    scr_1y_time = one_year_prior_admission[one_year_prior_admission.ENCOUNTERID.isin(complete_dfe[complete_dfe.ONE_WEEK_SCR.isna()].ENCOUNTERID.unique())] \n",
    "    scr_1y_time['days_before_admit']  = (scr_1y_time['ADMIT_DATE']  -  scr_1y_time['SPECIMEN_DATE']).dt.days\n",
    "    cohort_table['scr_1y_df'] = scr_1y_time\n",
    "    one_year_prior_admission = one_year_prior_admission.loc[:, pat_id_cols + ['RESULT_NUM']]\n",
    "    one_year_prior_admission = one_year_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].agg(aggfunc_1y).reset_index()\n",
    "    one_year_prior_admission.rename(columns = {'RESULT_NUM': 'ONE_YEAR_SCR'}, inplace = True)\n",
    "    \n",
    "    complete_df = complete_df.merge(one_year_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "    \n",
    "    complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), 'BASELINE_EST_2'] = \\\n",
    "                np.nanmin(complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), ['ONE_YEAR_SCR', 'ADMISSION_SCR']], axis = 1)\n",
    "\n",
    "    complete_df['BASELINE_NO_INVERT'] = \\\n",
    "                np.where(complete_df['BASELINE_EST_1'].isna(), complete_df['BASELINE_EST_2'], complete_df['BASELINE_EST_1'])\n",
    "\n",
    "    complete_dfe = complete_df.drop(['SPECIMEN_DATE', 'RESULT_NUM'],axis=1).drop_duplicates()\n",
    "    cohort_table['ONE_YEAR_SCR_YES'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna()).sum()\n",
    "    cohort_table['ONE_YEAR_SCR_NO'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.isna()).sum()\n",
    "    \n",
    "    cohort_table['ADMISSION_AND_1Y_SCR'] = (complete_dfe.ADMISSION_SCR.notna() & (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna())).sum()\n",
    "    cohort_table['ADMISSION_AND_1Y_SCR_MIN'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.BASELINE_EST_2.notna()).sum()\n",
    "    \n",
    "    cohort_table['ADMISSION_OR_1Y_SCR'] = (complete_dfe.ADMISSION_SCR.notna() | (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna())).sum()\n",
    "    \n",
    "    cohort_table['ONE_YEAR_SCR_ENC'] = complete_dfe[(complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna() & (complete_dfe['ONE_YEAR_SCR']==complete_dfe['BASELINE_EST_2']))]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ADMISSION_SCR_1Y_ENC'] = complete_dfe[(complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna() & (complete_dfe['ONE_YEAR_SCR']!=complete_dfe['BASELINE_EST_2']))]['ENCOUNTERID'].unique()\n",
    "\n",
    "    pat_to_invert = complete_df.loc[complete_df.BASELINE_NO_INVERT.isna(), pat_id_cols+['ADMIT_DATE', 'ADMISSION_SCR']]\n",
    "    \n",
    "    cohort_table['MDRD_TO_INVERT'] = pat_to_invert['ENCOUNTERID'].nunique()\n",
    "    cohort_table['MDRD_TO_INVERT_ENC'] = pat_to_invert['ENCOUNTERID'].unique()\n",
    "    pat_to_invert.drop_duplicates(subset=pat_id_cols, keep='first', inplace = True)\n",
    "    pat_dx = pat_to_invert.merge(dx.drop(['ENCOUNTERID', 'ADMIT_DATE'], axis = 1), \n",
    "                                              on = 'PATID', \n",
    "                                              how = 'left')\n",
    "    pat_dx = pat_dx[pat_dx.DX_DATE <= pat_dx.ADMIT_DATE]   \n",
    "    pat_dx = pat_dx.merge(pat_to_invert[['PATID', 'ENCOUNTERID']], \n",
    "                          on = ['PATID', 'ENCOUNTERID'], \n",
    "                          how = 'outer')\n",
    "    pat_dx['DFLT_eGFR'] = 75\n",
    "    \n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.3', 'N18.3']), 'DFLT_eGFR'] = 90/2\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.4', 'N18.4']), 'DFLT_eGFR'] = 45/2\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.5', 'N18.5']), 'DFLT_eGFR'] = 15/2\n",
    "    pat_def_egfr = pat_dx.groupby(pat_id_cols)['DFLT_eGFR'].min().reset_index()\n",
    "    \n",
    "    \n",
    "    cohort_table['ALL_CKD3_ENC'] = dx[(dx['DX'].isin(['585.3', 'N18.3']) )  & (dx['ENCOUNTERID'].isin(df_admit['ENCOUNTERID'].unique()))]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ALL_CKD4_ENC'] = dx[(dx['DX'].isin(['585.4', 'N18.4']) )  & (dx['ENCOUNTERID'].isin(df_admit['ENCOUNTERID'].unique()))]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ALL_CKD5_ENC'] = dx[(dx['DX'].isin(['585.5', 'N18.5']) )  & (dx['ENCOUNTERID'].isin(df_admit['ENCOUNTERID'].unique()))]['ENCOUNTERID'].unique()\n",
    "\n",
    "    cohort_table['MDRD_NOCKD'] = (pat_def_egfr['DFLT_eGFR'] == 75).sum()\n",
    "    \n",
    "    cohort_table['ADMISSION_OR_MDRD_NOCKD'] = (complete_dfe.ADMISSION_SCR.notna() | complete_dfe['ENCOUNTERID'].isin(pat_def_egfr[pat_def_egfr['DFLT_eGFR'] == 75]['ENCOUNTERID'].unique())).sum()\n",
    "    \n",
    "    cohort_table['MDRD_CKD3']  = (pat_def_egfr['DFLT_eGFR'] == 90/2).sum()\n",
    "    cohort_table['MDRD_CKD4']  = (pat_def_egfr['DFLT_eGFR'] == 45/2).sum()\n",
    "    cohort_table['MDRD_CKD5']  = (pat_def_egfr['DFLT_eGFR'] == 15/2).sum()\n",
    "        \n",
    "    pat_to_invert= pat_to_invert.merge(pat_def_egfr, on = pat_id_cols, how = 'left')\n",
    "    pat_to_invert['DFLT_eGFR'] = pat_to_invert['DFLT_eGFR'].fillna(75)\n",
    "\n",
    "    pat_to_invert['CKD345'] = pat_to_invert['DFLT_eGFR'] != 75\n",
    "    cohort_table['CKD345_ENC'] = pat_to_invert[pat_to_invert['CKD345']].ENCOUNTERID.unique()\n",
    "    pat_to_invert = pat_to_invert.merge(demo, on = pat_id_cols, how = 'left')\n",
    "    \n",
    "    KDIGO_baseline = np.array([\n",
    "        [1.5, 1.3, 1.2, 1.0],\n",
    "        [1.5, 1.2, 1.1, 1.0],\n",
    "        [1.4, 1.2, 1.1, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.8],\n",
    "        [1.2, 1.0, 0.9, 0.8]\n",
    "    ])\n",
    "    KDIGO_baseline = pd.DataFrame(KDIGO_baseline, columns = [\"Black males\", \"Other males\",\n",
    "                                                            \"Black females\", \"Other females\"],\n",
    "                                 index = [\"20-24\", \"25-29\", \"30-39\", \"40-54\", \"55-65\", \">65\"])    \n",
    "    \n",
    "    pat_to_invert.loc[~pat_to_invert['CKD345'], 'BASELINE_INVERT'] = pat_to_invert.loc[~pat_to_invert['CKD345'], :].apply(inverse_MDRD, args = (KDIGO_baseline,), axis = 1) \n",
    "    pat_to_invert.loc[pat_to_invert['CKD345'], 'BASELINE_INVERT'] = pat_to_invert.loc[pat_to_invert['CKD345'], :].apply(inverse_MDRD_raw, axis = 1) \n",
    "\n",
    "    pat_to_invert['BASELINE_EST_3'] = np.min(pat_to_invert[['ADMISSION_SCR', 'BASELINE_INVERT']], axis = 1)\n",
    "\n",
    "    cohort_table['MDRD_ENC'] = pat_to_invert[(~pat_to_invert['CKD345']) & (pat_to_invert['BASELINE_EST_3'] == pat_to_invert['BASELINE_INVERT'])]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ADMISSION_SCR_MDRD_ENC'] = pat_to_invert[(~pat_to_invert['CKD345']) & (pat_to_invert['BASELINE_EST_3'] != pat_to_invert['BASELINE_INVERT'])]['ENCOUNTERID'].unique()\n",
    "        \n",
    "    complete_df = complete_df.merge(pat_to_invert[pat_id_cols + ['BASELINE_EST_3', 'CKD345']], \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    complete_df['SERUM_CREAT_BASE'] = np.min(complete_df[['BASELINE_NO_INVERT', 'BASELINE_EST_3']], axis = 1)\n",
    "\n",
    "    if not keep_ckd:\n",
    "        complete_df = complete_df[~(complete_df['CKD345'] & complete_df['BASELINE_NO_INVERT'].isna())]\n",
    "\n",
    "    complete_df = complete_df.drop('CKD345', axis=1)\n",
    "        \n",
    "    # drop those still cannot find baseline\n",
    "    complete_df = complete_df.dropna(subset=['SERUM_CREAT_BASE'])\n",
    "\n",
    "    return complete_df.drop_duplicates(), cohort_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963ea302-66b6-46d3-9247-714758f2882f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eGFR_MDRD(df, scr_label):\n",
    "    # Adjust Scr for units; assuming Scr is given in mg/dL\n",
    "    Scr = df[scr_label]\n",
    "\n",
    "    # Coefficients for gender and race\n",
    "    gender_coeff = np.where(df['MALE'], 1, 0.742)\n",
    "    race_coeff = np.where(df['RACE_BLACK'], 1.212, 1)\n",
    "\n",
    "    # MDRD equation components\n",
    "    Scr_component = Scr ** -1.154\n",
    "    age_component = df['AGE'] ** -0.203\n",
    "\n",
    "    # eGFR calculation\n",
    "    eGFR = 175 * Scr_component * age_component * gender_coeff * race_coeff\n",
    "    return eGFR\n",
    "\n",
    "\n",
    "def eGFR_CKDEPI09(df, scr_label):\n",
    "    # Determine kappa and alpha based on 'MALE' column\n",
    "    kappa = np.where(df['MALE'], 0.9, 0.7)\n",
    "    alpha = np.where(df['MALE'],  -0.411, -0.329)\n",
    "\n",
    "    # Coefficients for gender\n",
    "    gender_coeff = np.where(df['MALE'], 1, 1.018)\n",
    "    race_coeff = np.where(df['RACE_BLACK'], 1.159, 1)\n",
    "    \n",
    "    # Calculate eGFR\n",
    "    Scr_over_kappa = df[scr_label] / kappa\n",
    "    min_term = np.where(Scr_over_kappa <= 1, Scr_over_kappa**alpha, 1)\n",
    "    max_term = np.where(Scr_over_kappa > 1, Scr_over_kappa**(-1.209), 1)\n",
    "    age_term = 0.993 ** df['AGE']\n",
    "\n",
    "    # eGFR calculation\n",
    "    eGFR = 141 * min_term * max_term * age_term * gender_coeff * race_coeff\n",
    "    return eGFR\n",
    "    \n",
    "\n",
    "def eGFR_CKDEPI21(df, scr_label):\n",
    "    # Determine kappa and alpha based on 'MALE' column\n",
    "    kappa = np.where(df['MALE'], 0.9, 0.7)\n",
    "    alpha = np.where(df['MALE'], -0.302, -0.241)\n",
    "\n",
    "    # Coefficients for gender\n",
    "    gender_coeff = np.where(df['MALE'], 1, 1.012)\n",
    "\n",
    "    # Calculate eGFR\n",
    "    Scr_over_kappa = df[scr_label] / kappa\n",
    "    min_term = np.where(Scr_over_kappa <= 1, Scr_over_kappa**alpha, 1)\n",
    "    max_term = np.where(Scr_over_kappa > 1, Scr_over_kappa**(-1.200), 1)\n",
    "    age_term = 0.9938 ** df['AGE']\n",
    "\n",
    "    # eGFR calculation\n",
    "    eGFR = 142 * min_term * max_term * age_term * gender_coeff\n",
    "    return eGFR\n",
    "    \n",
    "def ckd_staging(df, egfr_label):\n",
    "    conditions = [\n",
    "        (df[egfr_label] >= 90),\n",
    "        (df[egfr_label] >= 60) & (df[egfr_label] < 90),\n",
    "        (df[egfr_label] >= 45) & (df[egfr_label] < 60),\n",
    "        (df[egfr_label] >= 30) & (df[egfr_label] < 45),\n",
    "        (df[egfr_label] >= 15) & (df[egfr_label] < 30),\n",
    "        (df[egfr_label] < 15)\n",
    "    ]\n",
    "\n",
    "    choices = [0, 1, 2, 3, 4, 5]\n",
    "    ckd_stage = np.select(conditions, choices, default=np.nan) \n",
    "    return ckd_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d3dc74-0507-4f8e-94c5-55d818c0ef48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rrt(df_admit, file_paths):\n",
    "    px = pd.read_pickle(file_paths[0]+'AKI_PX.pkl')   \n",
    "\n",
    "    idx_transplant = np.logical_or(np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='CH',px['PX'].isin(['50300','50320','50323','50325','50327','50328','50329','50340','50360','50365','50370','50380'])),\n",
    "                           np.logical_and(px['PX_TYPE']=='09',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69']))),np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='9',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69'])),                       \n",
    "                           np.logical_and(px['PX_TYPE']=='10',px['PX'].isin(['0TY00Z0','0TY00Z1','0TY00Z2','0TY10Z0','0TY10Z1','0TY10Z2','0TB00ZZ','0TB10ZZ','0TT00ZZ','0TT10ZZ','0TT20ZZ']))))\n",
    "\n",
    "    idx_dialysis =(((px['PX_TYPE']=='CH') & (px['PX'].isin(['90935', '90937']))) |  \n",
    "                  ((px['PX_TYPE']=='CH') & (pd.to_numeric(px['PX'], errors='coerce').between(90940, 90999))) |   \n",
    "                  ((px['PX_TYPE']=='9') & (px['PX'].isin(['39.93','39.95','54.98', 'V45.11']))) | \n",
    "                  ((px['PX_TYPE']=='09') & (px['PX'].isin(['39.93','39.95','54.98', 'V45.11']))) |  \n",
    "                  ((px['PX_TYPE']=='10') & (px['PX'].isin(['5A1D00Z','5A1D60Z','5A1D70Z','5A1D80Z','5A1D90Z', 'Z99.2'])))) \n",
    " \n",
    "    rrt_stage =  px[idx_transplant | idx_dialysis] \n",
    "\n",
    "    rrt_stage = rrt_stage[['PATID','ENCOUNTERID','PX_DATE']]\n",
    "    rrt_stage.columns = ['PATID','ENCOUNTERID','RRT_ONSET_DATE']\n",
    "\n",
    "    rrt_stage = rrt_stage.merge(df_admit, on=['PATID', 'ENCOUNTERID'], how='inner')\n",
    "    rrt_stage['RRT_SINCE_ADMIT'] = (rrt_stage['RRT_ONSET_DATE']-rrt_stage['ADMIT_DATE']).dt.total_seconds()/(3600*24)\n",
    "    rrt_stage = rrt_stage.loc[rrt_stage[['ENCOUNTERID', 'RRT_SINCE_ADMIT']].groupby('ENCOUNTERID').idxmin().reset_index()['RRT_SINCE_ADMIT']]\n",
    "    rrt_stage.drop('ADMIT_DATE', axis = 1, inplace = True)\n",
    "    return rrt_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7876c218-312f-48a8-b136-9d64a38f2d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def determine_initial_aki_stage(row):\n",
    "    # Extract the AKI onset days\n",
    "    aki_days = {\n",
    "        1: row['AKI1_SINCE_ADMIT'],\n",
    "        2: row['AKI2_SINCE_ADMIT'],\n",
    "        3: row['AKI3_SINCE_ADMIT']\n",
    "    }\n",
    "    \n",
    "    # Remove NaN values\n",
    "    aki_days = {key: val for key, val in aki_days.items() if not pd.isnull(val)}\n",
    "    \n",
    "    if not aki_days:\n",
    "        return np.nan\n",
    "    \n",
    "    # Find the minimum value and handle ties by prioritizing higher stages\n",
    "    min_value = min(aki_days.values())\n",
    "    highest_stage = max(stage for stage, day in aki_days.items() if day == min_value)\n",
    "    \n",
    "    return highest_stage\n",
    "\n",
    "def get_aki_onset(df_scr, df_admit, df_rrt, df_baseline, aki_criteria = 'either'):\n",
    "    xxx = df_scr.copy()\n",
    "    yyy = df_admit.copy()\n",
    "    \n",
    "    # Filter out the CKD patients that does not have baseline \n",
    "    valid_combinations = df_baseline[['ENCOUNTERID', 'PATID']].drop_duplicates()\n",
    "    xxx = xxx.merge(valid_combinations, on=['ENCOUNTERID', 'PATID'], how='inner')\n",
    "    yyy = yyy.merge(valid_combinations, on=['ENCOUNTERID', 'PATID'], how='inner')\n",
    "    \n",
    "    #\n",
    "    zzz = df_baseline[['PATID', 'ENCOUNTERID', 'SERUM_CREAT_BASE']].drop_duplicates()\n",
    "    zzz.columns= ['PATID', 'ENCOUNTERID',  'RESULT_NUM_BASE_7d']\n",
    "    xxx = xxx.merge(zzz, on = ['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "    zzz2 = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']].groupby(['PATID', 'ENCOUNTERID']).rolling('2d', on='SPECIMEN_DATE').min().reset_index()\n",
    "    zzz2 = zzz2[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    "    zzz2.columns= ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM_BASE_2d']\n",
    "    xxx = xxx.merge(zzz2, on = ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'], how='left')\n",
    "\n",
    "    # Check condition for AKI1\n",
    "    #1.5 increase in 7 days\n",
    "    xxx['AKI1.5'] = (xxx['RESULT_NUM']>=1.5*xxx['RESULT_NUM_BASE_7d']) & (xxx['DAYS_SINCE_ADMIT']>=0) \n",
    "    #0.3 increase in 48 hours\n",
    "    xxx['AKI0.3'] = (xxx['RESULT_NUM']-xxx['RESULT_NUM_BASE_2d']>=0.3) & (xxx['DAYS_SINCE_ADMIT']>=0)      \n",
    "    \n",
    "    if aki_criteria == '2d':\n",
    "        xxx = xxx[xxx['AKI0.3']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    " \n",
    "    elif aki_criteria == '7d':\n",
    "        xxx = xxx[xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "\n",
    "    elif aki_criteria == 'both':\n",
    "        xxx = xxx[xxx['AKI0.3'] & xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "\n",
    "    else:\n",
    "        xxx = xxx[xxx['AKI0.3'] | xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "\n",
    "\n",
    "    xxx['AKI1_SINCE_ADMIT'] = xxx['DAYS_SINCE_ADMIT'].copy()\n",
    "    xxx['AKI1_DATE'] = xxx['SPECIMEN_DATE'].copy()\n",
    "    xxx['AKI1_SCR'] = xxx['RESULT_NUM'].copy()\n",
    "    xxx['SCR_BASELINE'] = xxx['RESULT_NUM_BASE'].copy()\n",
    "    xxx['SCR_REFERENCE'] = xxx['RESULT_NUM_BASE_2d'].copy()\n",
    "    xxx['AKI1_7D'] = xxx['AKI1.5'].copy()\n",
    "    xxx['AKI1_2D'] = xxx['AKI0.3'].copy()\n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SCR_BASELINE', 'SCR_REFERENCE',  'AKI1_DATE', 'AKI1_SCR', 'AKI1_SINCE_ADMIT', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "    # Check condition for AKI2: 2.0x - <3.0x\n",
    "    aki2 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki2 = aki2[aki2['SPECIMEN_DATE']>=aki2['AKI1_DATE']]\n",
    "    aki2 = aki2[aki2['RESULT_NUM']>=2*aki2['SCR_BASELINE']]\n",
    "    aki2 = aki2.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki2['AKI2_SINCE_ADMIT'] = aki2['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki2['AKI2_DATE'] = aki2['SPECIMEN_DATE'].copy()\n",
    "    aki2['AKI2_SCR'] = aki2['RESULT_NUM'].copy()\n",
    "    aki2 = aki2[['PATID', 'ENCOUNTERID', 'AKI2_DATE', 'AKI2_SCR', 'AKI2_SINCE_ADMIT']]\n",
    "    \n",
    "    # Check condition for AKI3: SCR >= 3.0x Baseline\n",
    "    aki3 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki3 = aki3[aki3['SPECIMEN_DATE']>=aki3['AKI1_DATE']]\n",
    "    aki3 = aki3[(aki3['RESULT_NUM']>=3*aki3['SCR_BASELINE']) | (aki3['RESULT_NUM']>=4)]\n",
    "    aki3 = aki3.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki3['AKI3_SINCE_ADMIT'] = aki3['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki3['AKI3_DATE'] = aki3['SPECIMEN_DATE'].copy()\n",
    "    aki3['AKI3_SCR'] = aki3['RESULT_NUM'].copy()\n",
    "    aki3 = aki3[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    # Check condition for AKI3: initiation of RRT\n",
    "    #df_rrt = get_rrt(path, ext, sep, yyy)\n",
    "    rrt = df_rrt.merge(xxx[['PATID', 'ENCOUNTERID', 'AKI1_DATE']], on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    rrt = rrt[rrt['RRT_ONSET_DATE'] >= rrt['AKI1_DATE']]\n",
    "    aki3b =  aki3.merge(rrt, on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "    cond_rrt = (aki3b['RRT_SINCE_ADMIT'] < aki3b['AKI3_SINCE_ADMIT']) | (aki3b['AKI3_SINCE_ADMIT'].isna() & aki3b['RRT_SINCE_ADMIT'].notna())\n",
    "    aki3b.loc[cond_rrt, 'AKI3_SINCE_ADMIT'] = aki3b.loc[cond_rrt, 'RRT_SINCE_ADMIT']\n",
    "    aki3b.loc[cond_rrt, 'AKI3_DATE'] = aki3b.loc[cond_rrt, 'RRT_ONSET_DATE']\n",
    "    \n",
    "    aki3_all = aki3b[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    # Merge AKI staging information\n",
    "    onset = xxx.merge(aki2, on=['PATID', 'ENCOUNTERID'], how='outer').merge(aki3_all, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    onset = onset.merge(yyy, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "    onset.columns = onset.columns.str.upper()\n",
    "    onset['ONSET_DATE'] = onset['AKI1_DATE'].copy()  \n",
    "    onset['SCR_ONSET'] = onset['AKI1_SCR'].copy() \n",
    "    \n",
    "    onset['DISCHARGE_SINCE_ONSET'] = (onset['DISCHARGE_DATE'] - onset['ONSET_DATE']).dt.days\n",
    "    \n",
    "    onset = onset[['PATID','ENCOUNTERID', 'ADMIT_DATE', 'DISCHARGE_DATE', \n",
    "                   'ONSET_DATE', 'AKI1_SINCE_ADMIT', 'AKI2_SINCE_ADMIT', \n",
    "                   'AKI3_SINCE_ADMIT',  'DISCHARGE_SINCE_ONSET','SCR_ONSET', \n",
    "                   'SCR_BASELINE',  'SCR_REFERENCE', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "    onset['FLAG'] = (onset['AKI2_SINCE_ADMIT'].notna()) | (onset['AKI3_SINCE_ADMIT'].notna())\n",
    "    onset['ONSET_SINCE_ADMIT'] = onset['AKI1_SINCE_ADMIT'].copy()  #onset[['AKI1_SINCE_ADMIT', 'AKI2_SINCE_ADMIT', 'AKI3_SINCE_ADMIT']].min(axis=1)\n",
    "    \n",
    "    #Generate onset staging by taking the first stage\n",
    "    onset['AKI_STAGE'] = 0\n",
    "    filter_aki3 = onset['AKI3_SINCE_ADMIT'].notna()\n",
    "    filter_aki2 = onset['AKI2_SINCE_ADMIT'].notna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    filter_aki1 = onset['AKI1_SINCE_ADMIT'].notna() & onset['AKI2_SINCE_ADMIT'].isna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    \n",
    "    onset.loc[filter_aki3, 'AKI_STAGE'] = 3\n",
    "    onset.loc[filter_aki2, 'AKI_STAGE'] = 2\n",
    "    onset.loc[filter_aki1, 'AKI_STAGE'] = 1\n",
    "    \n",
    "    \n",
    "    # Determine the initial AKI stage by finding the column with the smallest onset day\n",
    "    onset['AKI_INIT_STG'] = onset.apply(determine_initial_aki_stage, axis=1)\n",
    "    \n",
    "    return onset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ffbe4-314d-4856-8c23-df6d8eff385d",
   "metadata": {},
   "source": [
    "### Step 2: Get AKI Recovery Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "348c75b0-4744-4667-864c-1a8679b6b302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_filter_scr(onset, file_paths):\n",
    "    xxx = pd.read_pickle(file_paths[0]+'AKI_LAB_SCR.pkl') \n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE',  'RESULT_NUM']] \n",
    "    \n",
    "    xxx = xxx.groupby(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE']).mean()\n",
    "    xxx = xxx.sort_values(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'])\n",
    "    xxx = xxx.reset_index()\n",
    "    # merge with onset data\n",
    "    xxx = onset.merge(xxx, on = ['ENCOUNTERID', 'PATID'], how='inner')\n",
    "    xxx['DAYS_SINCE_ONSET'] = (xxx['SPECIMEN_DATE']-xxx['ONSET_DATE']).dt.days\n",
    "    return xxx\n",
    "\n",
    "def akirecovery(onset, scr):\n",
    "    onset2 = onset[['PATID', 'ENCOUNTERID', 'DISCHARGE_DATE', 'DISCHARGE_SINCE_ONSET']]\n",
    "    df = onset2.merge(scr, on = ['PATID', 'ENCOUNTERID'], how = 'right')\n",
    "    df2 = df[(df['ONSET_DATE'] < df['SPECIMEN_DATE']) & (df['SPECIMEN_DATE'] <= df['DISCHARGE_DATE'])].sort_values('SPECIMEN_DATE')\n",
    "    df3 = df2.groupby(['PATID', 'ENCOUNTERID']).last().reset_index()\n",
    "    df3['AKI_RCV'] = (((df3['RESULT_NUM'] < (1.5 * df3['SCR_BASELINE']) ) &  df3['AKI1_7D']) \n",
    "                     |((df3['RESULT_NUM'] <  (0.3 + df3['SCR_REFERENCE']) ) & df3['AKI1_2D']))\n",
    "\n",
    "    return df3[['PATID', 'ENCOUNTERID', 'AKI_RCV']]\n",
    "\n",
    "# Generate AKI resolving Status (Kellum Definition)\n",
    "def akidisappearing(dall):\n",
    "    dall['AKI_disappearing']=False\n",
    "\n",
    "    dall['ddays']= (dall['charttime']-dall['onsettime'])\n",
    "    dall['dvalues_rat'] = dall['value']/dall['baseline']\n",
    "    dall['dvalues_lvl'] = dall['value']-dall['reference']\n",
    "    filter_rat = (dall['ddays']>pd.Timedelta(value=0, unit='s')) & (dall['dvalues_rat']<1.5) & dall['aki1_7d']\n",
    "    filter_lvl = (dall['ddays']>pd.Timedelta(value=0, unit='s')) & (dall['dvalues_lvl']<0.3) & dall['aki1_2d']\n",
    "    \n",
    "    dall.loc[(filter_rat | filter_lvl),'AKI_disappearing']=True\n",
    "\n",
    "    dall['disappeared']=dall['AKI_disappearing'].copy()\n",
    "\n",
    "    dmaxRelv = dall[['subject_id', 'hadm_id', 'AKI_disappearing']].groupby(['subject_id', 'hadm_id']).max().reset_index()\n",
    "    dmaxRelv.columns = ['subject_id', 'hadm_id', 'max_AKI_disappearing']\n",
    "    dall = dall.merge(dmaxRelv, left_on=['subject_id', 'hadm_id'], right_on=['subject_id', 'hadm_id'], how='left')\n",
    "    dall['AKI_disappearing'] = dall['max_AKI_disappearing']\n",
    "    \n",
    "    # Filter where AKI_disappearing is True and then get the first occurrence for each group\n",
    "    first_resolv = dall[dall['disappeared'] == True][['subject_id', 'hadm_id','charttime']].sort_values('charttime').groupby(['subject_id', 'hadm_id']).first().reset_index()\n",
    "    dall2 = dall[['subject_id', 'hadm_id', 'AKI_disappearing']].groupby(['subject_id', 'hadm_id'])['AKI_disappearing'].max().reset_index()\n",
    "    df_resolv =  dall2.merge(first_resolv, on=['subject_id', 'hadm_id'], how='left')\n",
    "    \n",
    "    df_resolv.columns = ['PATID', 'ENCOUNTERID', 'AKI_DISP', 'FIRST_DISP_TIME']\n",
    "    return dall.drop('max_AKI_disappearing',axis = 1), df_resolv\n",
    "\n",
    "def akireverting(dall):\n",
    "    result = dall.sort_values(['subject_id', 'hadm_id','charttime']).groupby(['subject_id', 'hadm_id']).apply(check_reverted_and_relapsed).reset_index()\n",
    "    \n",
    "    result.columns = ['PATID', 'ENCOUNTERID', 'AKI_RVRT',  'FIRST_RVRT_TIME', 'AKI_RELP', 'FIRST_RELP_TIME']\n",
    "    return result\n",
    "\n",
    "def interp_and_check_next_day(filtered_group, row):\n",
    "    group_nexttime = filtered_group.iloc[0]\n",
    "    group_nextday = group_nexttime \n",
    "\n",
    "    nxttime = group_nexttime['charttime']  \n",
    "    curtime = row['charttime']\n",
    "    group_nextday['dvalues_rat'] = row['dvalues_rat'] + (group_nexttime['dvalues_rat'] - row['dvalues_rat']) / ((nxttime - curtime).total_seconds() / (60 * 60 * 24) )\n",
    "    group_nextday['dvalues_lvl'] = row['dvalues_lvl'] + (group_nexttime['dvalues_lvl'] - row['dvalues_lvl']) / ((nxttime - curtime).total_seconds() / (60 * 60 * 24) )\n",
    "\n",
    "    group_nextday['ddays'] = row['ddays'] + pd.Timedelta(days=1)\n",
    "\n",
    "    rat_condition = (group_nextday['ddays']>pd.Timedelta(value=0, unit='s')) & (group_nextday['dvalues_rat']<1.5) & row['aki1_7d']\n",
    "    lvl_condition = (group_nextday['ddays']>pd.Timedelta(value=0, unit='s')) & (group_nextday['dvalues_lvl']<0.3) & row['aki1_2d']\n",
    "    next_disappeared = (rat_condition | lvl_condition)\n",
    "    \n",
    "    return next_disappeared\n",
    "\n",
    "        \n",
    "def check_reverted_and_relapsed(group):\n",
    "    group['reverted'] = False\n",
    "    for i, row in group.iterrows():\n",
    "        if row['disappeared']:\n",
    "            next_time = group[group['charttime'] > row['charttime']]['charttime'].min()\n",
    "            next_day = row['charttime'] + pd.Timedelta(days=1)\n",
    "            if pd.isna(next_time) :\n",
    "                next_disappeared = True\n",
    "                \n",
    "                if next_disappeared:\n",
    "                    group.loc[i, 'reverted'] = True\n",
    "            elif next_time > next_day :\n",
    "                filter_nexttime = (group['charttime'] == next_time)\n",
    "                #next_disappeared = group[filter_nexttime]['disappeared'].iloc[0] if not pd.isna(next_time) else \n",
    "                next_disappeared = interp_and_check_next_day(group[filter_nexttime], row)\n",
    "                \n",
    "                if next_disappeared:\n",
    "                    group.loc[i, 'reverted'] = True\n",
    "            else:\n",
    "                mask = (group['charttime'] > row['charttime']) & (group['charttime'] <= next_day)\n",
    "                if group.loc[mask, 'disappeared'].all():\n",
    "                    group.loc[i, 'reverted'] = True\n",
    "\n",
    "    first_reverted_time = group.loc[group['reverted'], 'charttime'].min() if group['reverted'].any() else None\n",
    "    group_reverted = group['reverted'].any()\n",
    "\n",
    "    # Initialize relapsed as False\n",
    "    relapsed = False\n",
    "    first_relapsed_time = None\n",
    "\n",
    "    # If group is reverted, check for relapse\n",
    "    if group_reverted and first_reverted_time is not None:\n",
    "        # Filter for records after first_reverted_time\n",
    "        post_reverted_records = group[group['charttime'] > first_reverted_time]\n",
    "        # Check if any of these records has 'disappeared' as False\n",
    "        if not post_reverted_records.empty and (post_reverted_records['disappeared'] == False).any():\n",
    "            relapsed = True\n",
    "            first_relapsed_time = post_reverted_records[post_reverted_records['disappeared'] == False]['charttime'].min()\n",
    "\n",
    "    return pd.Series({\n",
    "        'reverted': group_reverted,\n",
    "        'first_revert_time': first_reverted_time,\n",
    "        'relap': relapsed,\n",
    "        'first_relap_time': first_relapsed_time\n",
    "    })\n",
    "\n",
    "\n",
    "def get_aki_reverting(scr, onset):\n",
    "    scr = scr[scr['DAYS_SINCE_ONSET']>=0]\n",
    "    scr = scr[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM', 'DAYS_SINCE_ONSET', \n",
    "               'ONSET_DATE', 'SCR_BASELINE', 'SCR_REFERENCE', 'AKI1_2D', 'AKI1_7D']]\n",
    "    \n",
    "    aki_recov = akirecovery(onset, scr)\n",
    "    \n",
    "    scr.columns = ['subject_id', 'hadm_id', 'charttime', 'value', 'onset_day',\n",
    "                   'onsettime', 'baseline', 'reference', 'aki1_2d', 'aki1_7d']\n",
    "\n",
    "    xxx, df_resolv = akidisappearing(dall = scr)\n",
    "    xxx = akireverting(dall = xxx)\n",
    "    \n",
    "\n",
    "    # Merge the results on PATID and ENCOUNTERID\n",
    "    df_revert = xxx.merge(df_resolv,  \n",
    "                   on=['PATID', 'ENCOUNTERID'],\n",
    "                   how = 'left')\n",
    "\n",
    "    onset = onset.merge(df_revert,\n",
    "                        on=['PATID', 'ENCOUNTERID'], \n",
    "                        how='left')\n",
    "    \n",
    "    onset = onset.merge(aki_recov, on = ['PATID', 'ENCOUNTERID'], how = 'left')\n",
    "    onset['AKI_RCV'].fillna(False, inplace = True)\n",
    "    \n",
    "    onset['DISP_SINCE_ONSET'] = (onset['FIRST_DISP_TIME'] - onset['ONSET_DATE']).dt.days\n",
    "    onset['RVRT_SINCE_ONSET'] = (onset['FIRST_RVRT_TIME'] - onset['ONSET_DATE']).dt.days\n",
    "    onset['RELP_SINCE_ONSET'] = (onset['FIRST_RELP_TIME'] - onset['ONSET_DATE']).dt.days\n",
    "    \n",
    "    onset['AKI_ERVRT']   =  onset['AKI_RVRT'] & (onset['FIRST_RVRT_TIME'] <= (onset['ONSET_DATE'] + pd.Timedelta(days=7)))\n",
    "    onset['AKI_ESRVRT']  = (onset['AKI_ERVRT'])  & (~onset['AKI_RELP'])\n",
    "    onset['AKI_LSRVRT']  = (~onset['AKI_ERVRT']) & onset['AKI_RVRT']  & (~onset['AKI_RELP'])\n",
    "    onset['AKI_NRVRT']   =  ~onset['AKI_RVRT']\n",
    "    onset['AKI_RLPRCV']  = (onset['AKI_RELP']) & (onset['AKI_RCV'])\n",
    "    onset['AKI_RLPNRCV'] = (onset['AKI_RELP']) & (~onset['AKI_RCV'])\n",
    "    onset['AKI_STATUS']  = (0*onset['AKI_NRVRT'] + \n",
    "                                         1*onset['AKI_ESRVRT'] + \n",
    "                                         2*onset['AKI_LSRVRT'] +  \n",
    "                                         3*onset['AKI_RLPRCV'] + \n",
    "                                         4*onset['AKI_RLPNRCV'])\n",
    "    \n",
    "    return onset.drop_duplicates()\n",
    "\n",
    "def akiresolving(dall, time, dlevel = 0.3, ratio = 0.75):\n",
    "    time_window = (dall['onset_day']<=3) & (dall['onset_day']>=time) \n",
    "    dall = dall[time_window].reset_index().drop('index',axis=1)\n",
    "    dall2 = dall.sort_values(['value', 'charttime'], ascending=[False, True])[['subject_id', 'hadm_id', 'charttime', 'value']]\n",
    "    max_idx = dall2.groupby(['subject_id', 'hadm_id'])['value'].idxmax()\n",
    "    dmax_sCr = dall2.loc[max_idx]\n",
    "    dmax_sCr.columns = ['subject_id', 'hadm_id', 'charttime_max', 'value_max']\n",
    "    dall = dall.merge(dmax_sCr, left_on=['subject_id', 'hadm_id'], right_on=['subject_id', 'hadm_id'], how='left')\n",
    "    # redefine time windows\n",
    "    time_window = (dall['onset_day']<=3) & (dall['onset_day']>=time)\n",
    "    dall['AKI_resolving']=0\n",
    "   \n",
    "    dall['ddays']= (dall['charttime']-dall['charttime_max'])\n",
    "    dall['dvalues'] = dall['value_max']-dall['value']\n",
    "    dall['dvalues2'] = dall['value']/dall['value_max']\n",
    "    # within 72 hrs, which means onset_day equals 1, 2, or 3\n",
    "    filter_lvl = (dall['ddays']>pd.Timedelta(value=0, unit='s')) & time_window & (dall['dvalues']>=dlevel)\n",
    "    filter_rat = (dall['ddays']>pd.Timedelta(value=0, unit='s')) & time_window & (dall['dvalues2']<=ratio)\n",
    "    dall.loc[filter_lvl,'AKI_resolving']=1\n",
    "    dall.loc[filter_rat,'AKI_resolving']=1\n",
    "    dall=dall.drop(['ddays', 'dvalues', 'dvalues2'],axis=1)\n",
    "    dall['AKI_resolving2']=dall['AKI_resolving'].copy()\n",
    "\n",
    "    dmaxRelv = dall[['subject_id', 'hadm_id', 'AKI_resolving']].groupby(['subject_id', 'hadm_id']).max().reset_index()\n",
    "    dmaxRelv.columns = ['subject_id', 'hadm_id', 'max_AKI_resolving']\n",
    "    dall = dall.merge(dmaxRelv, left_on=['subject_id', 'hadm_id'], right_on=['subject_id', 'hadm_id'], how='left')\n",
    "    dall['AKI_resolving'] = dall['max_AKI_resolving']\n",
    "    return dall.drop('max_AKI_resolving',axis = 1)\n",
    "\n",
    "def akiresolvingsustain(dall, time):\n",
    "    \n",
    "    dresolv = dall[dall['AKI_resolving2']==1]\n",
    "    dresolv = dresolv[['subject_id', 'hadm_id', 'charttime']].sort_values('charttime').groupby(['subject_id', 'hadm_id']).first().reset_index()\n",
    "    dresolv.columns = ['subject_id', 'hadm_id', 'charttime_resolv']\n",
    "\n",
    "    dall = dall.merge(dresolv, left_on=['subject_id', 'hadm_id'], right_on=['subject_id', 'hadm_id'], how='left')\n",
    "    time_window = (dall['onset_day']<=3) & (dall['onset_day']>=time) \n",
    "    dall['after_ddays'] = ((dall['charttime']-dall['charttime_resolv']) > pd.Timedelta(value=0, unit='s')) & time_window \n",
    "\n",
    "    dresolv2 = dall[dall['after_ddays']][['subject_id', 'hadm_id', 'AKI_resolving2']].groupby(['subject_id', 'hadm_id']).min().reset_index()\n",
    "    dresolv2.columns = ['subject_id', 'hadm_id', 'sustain']\n",
    "\n",
    "    dall = dall.merge(dresolv2, left_on=['subject_id', 'hadm_id'], right_on=['subject_id', 'hadm_id'], how='left')\n",
    "    dall['sustain'].fillna(True, inplace = True)\n",
    "    return dall\n",
    "\n",
    "def get_aki_resolving(scr, onset, time = 0, dlevel = 0.3, ratio = 0.75):\n",
    "    scr = scr[scr['DAYS_SINCE_ONSET']>=0]\n",
    "    scr = scr[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM', 'DAYS_SINCE_ONSET']]\n",
    "    scr.columns = ['subject_id', 'hadm_id', 'charttime', 'value', 'onset_day']\n",
    "\n",
    "    scr = akiresolving(dall = scr, time = time, dlevel = dlevel, ratio = ratio)\n",
    "    scr = akiresolvingsustain(dall = scr, time = time)\n",
    "    scr['AKI_sustain'] = scr['AKI_resolving']*scr['sustain']\n",
    "\n",
    "    scr['PATID'] = scr['subject_id']\n",
    "    scr['ENCOUNTERID'] = scr['hadm_id']\n",
    "\n",
    "    scr = scr[['PATID', 'ENCOUNTERID', 'AKI_resolving', 'AKI_sustain']].groupby(['PATID', 'ENCOUNTERID']).sum().reset_index()\n",
    "    scr['AKI_TRIGG'] = scr['AKI_resolving']>0\n",
    "    scr['AKI_RESOL'] = scr['AKI_sustain']>0\n",
    "    onset = onset.merge(scr[['PATID', 'ENCOUNTERID',  'AKI_TRIGG', 'AKI_RESOL']],on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    return onset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a714ca6d-2d74-4a49-92a1-9e807233bbb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_onset_and_recovery(file_paths, aggfunc_7d, aggfunc_1y, keep_ckd):\n",
    "    file_paths = get_data_file_path(base_path, site_name)\n",
    "    df_scr, df_admit = load_onset_data(file_paths)\n",
    "    df_baseline, cohort_table = get_scr_baseline_new(df_scr, df_admit, file_paths, aggfunc_7d, aggfunc_1y, keep_ckd)\n",
    "    df_rrt = get_rrt(df_admit, file_paths)\n",
    "\n",
    "    onset = get_aki_onset(df_scr, df_admit, df_rrt, df_baseline)\n",
    "    onset.to_pickle(file_paths[1] + 'onset.pkl')\n",
    "\n",
    "    df_scr = load_and_filter_scr(onset, file_paths)\n",
    "\n",
    "    onset_resol = get_aki_resolving(scr = df_scr, onset = onset)\n",
    "    onset_revert = get_aki_reverting(df_scr, onset)\n",
    "\n",
    "    onset_combined = onset_revert.merge(onset_resol[['PATID', 'ENCOUNTERID', 'AKI_TRIGG', 'AKI_RESOL']], \n",
    "                                     on = ['PATID', 'ENCOUNTERID'], \n",
    "                                     how = 'left').drop_duplicates()\n",
    "\n",
    "    onset_combined.to_pickle(file_paths[1]+'outcome.pkl')\n",
    "    print(f\"Finish generating AKI onset and recovery for {site_name}.\", flush = True)\n",
    "    return onset, cohort_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48030c3e-a47a-4cd4-9a51-c98abe079f06",
   "metadata": {},
   "source": [
    "#### Step 3: Extract and Process Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b412f2e-9d9d-4bd2-a0f4-3d7621d134ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_create_timeline(file_paths, filename, date_var, event_type, lapsed_day):\n",
    "    input_path = file_paths[0]\n",
    "    output_path = file_paths[1]\n",
    "    df = pd.read_pickle(input_path + filename +'.pkl')\n",
    "    onset = pd.read_pickle(output_path + 'onset.pkl')\n",
    "    \n",
    "    if event_type.upper() == 'PROGNOSIS':\n",
    "        onset = onset[onset['AKI_INIT_STG'] > 0]\n",
    "    \n",
    "    df_merged = onset.merge(df, on = ['PATID', 'ENCOUNTERID'], how = 'inner')\n",
    "\n",
    "    if  ('DAYS_SINCE_ADMIT' not in df_merged.columns) or (df_merged[date_var].isna().mean() <= df_merged.get('DAYS_SINCE_ADMIT', pd.Series()).isna().mean()):\n",
    "        df_merged['DAYS_SINCE_ADMIT'] = (df_merged[date_var]-df_merged['ADMIT_DATE']).dt.days\n",
    "        \n",
    "        \n",
    "    if (event_type.upper() == 'ONSET') | (event_type.upper() == 'PROGNOSIS'):\n",
    "        df_merged['MIN_ONSET_DISCHARGE_DATE'] = np.where(df_merged['ONSET_DATE'].notna(), \n",
    "                                                         df_merged['ONSET_DATE'], \n",
    "                                                         df_merged['DISCHARGE_DATE'])\n",
    "        \n",
    "        if df_merged[date_var].isna().mean() <= df_merged['DAYS_SINCE_ADMIT'].isna().mean():\n",
    "            df_merged['DAYS_SINCE_EVENT'] = (df_merged[date_var]-df_merged['MIN_ONSET_DISCHARGE_DATE']).dt.days\n",
    "        else: \n",
    "            print(f\"No date variable for {filename} from site {file_paths[0].split('/')[-2]}.\", flush = True)\n",
    "            df_merged['DAYS_SINCE_EVENT'] = df_merged['DAYS_SINCE_ADMIT'] - (df_merged['MIN_ONSET_DISCHARGE_DATE'] - df_merged['ADMIT_DATE']).dt.days\n",
    "            \n",
    "\n",
    "    if filename == 'AKI_DX':\n",
    "        df_merged = df_merged[(df_merged['DAYS_SINCE_ADMIT']>=(-365.25)/2) & (df_merged['DAYS_SINCE_ADMIT'] < 0)]\n",
    "    else:\n",
    "        df_merged = df_merged[(df_merged['DAYS_SINCE_ADMIT']>=0) & (df_merged['DAYS_SINCE_EVENT'] < lapsed_day)]\n",
    "        \n",
    "    return df_merged, onset\n",
    "    \n",
    "def replace_outliers(df, var_id, col_to_filter, q_l = 0.01, q_u = 0.99):\n",
    "    def replace_group_outliers(group):\n",
    "        lower = group[col_to_filter].quantile(q_l)\n",
    "        upper = group[col_to_filter].quantile(q_u)\n",
    "        group[col_to_filter] = group[col_to_filter].apply(lambda x: lower if x < lower else (upper if x > upper else x))\n",
    "        return group\n",
    "\n",
    "    if var_id is None:\n",
    "        df = replace_group_outliers(df)\n",
    "    else:\n",
    "        df = df.groupby(var_id).apply(replace_group_outliers).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_sparse_features(df, df_onset, var_id, trim=0.05):\n",
    "    table_count = df[['PATID', 'ENCOUNTERID', var_id]].drop_duplicates()\n",
    "    df_count = table_count.groupby(var_id).count()\n",
    "    df_count['percentage'] = df_count['ENCOUNTERID'] / df_onset.shape[0]\n",
    "    df_count = df_count.loc[(df_count['percentage'] >= trim)]\n",
    "    df_count = df_count.reset_index()\n",
    "    df_count = df_count.drop(['PATID', 'ENCOUNTERID', 'percentage'], axis=1)\n",
    "    df_filtered = df_count.merge(df, on=[var_id], how='left')\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d41c0182-642b-4eea-ba91-224eea19196d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_demo(file_paths):\n",
    "    demo = pd.read_pickle(file_paths[0]+'AKI_DEMO.pkl')\n",
    "    demo['MALE'] = demo['SEX'] == 'M'\n",
    "    demo['HISPANIC'] = demo['HISPANIC'] == 'Y'\n",
    "    demo['RACE_WHITE'] = demo['RACE'] == '05'\n",
    "    demo['RACE_BLACK'] = demo['RACE'] == '03'\n",
    "    demo = demo[['PATID', 'ENCOUNTERID', 'AGE', 'MALE', 'RACE_WHITE', 'RACE_BLACK', 'HISPANIC']]\n",
    "    demo = demo.drop_duplicates()\n",
    "    demo.to_pickle(file_paths[1]+'demo.pkl')\n",
    "    print(f\"Finished processing demo data from {file_paths[0].split('/')[-2]}.\", flush = True)\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8e95631-7cb7-4f0f-876e-9d776d6a79dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_vital(file_paths, event_type, lapsed_day):\n",
    "    # Load data and calculate timelines\n",
    "    vital, onset = load_and_create_timeline(file_paths, 'AKI_VITAL', 'MEASURE_DATE', event_type, lapsed_day)\n",
    "                        \n",
    "    vital = vital[(vital['ORIGINAL_BMI'].notna()) | (vital['HT'].notna()) | (vital['WT'].notna())\n",
    "                  | (vital['SYSTOLIC'].notna()) | (vital['DIASTOLIC'].notna())\n",
    "                  | (vital['SMOKING'].notna())]\n",
    "\n",
    "    vital_r = vital[['PATID', 'ENCOUNTERID', 'MEASURE_DATE', \n",
    "                     'ORIGINAL_BMI', 'HT', 'WT', 'SMOKING', 'SYSTOLIC', 'DIASTOLIC',\n",
    "                     'DAYS_SINCE_ADMIT', 'DAYS_SINCE_EVENT']]\n",
    "\n",
    "    # Replace the outliers\n",
    "    vital_r = replace_outliers(vital_r, None, 'ORIGINAL_BMI')\n",
    "    vital_r = replace_outliers(vital_r, None, 'HT')\n",
    "    vital_r = replace_outliers(vital_r, None, 'WT')\n",
    "    vital_r = replace_outliers(vital_r, None, 'SYSTOLIC')\n",
    "    vital_r = replace_outliers(vital_r, None, 'DIASTOLIC')\n",
    "\n",
    "    vital_mean = vital_r[['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT',\n",
    "                          'ORIGINAL_BMI', 'HT', 'WT', \n",
    "                          'SYSTOLIC', 'DIASTOLIC']].groupby(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT']).mean().reset_index()\n",
    "    \n",
    "    vital_bmi = vital_mean[['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT', 'ORIGINAL_BMI']].dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT'])\n",
    "    vital_wt = vital_mean[['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT', 'WT']].dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT'])\n",
    "    vital_ht = vital_mean[['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT', 'HT']].dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT'])\n",
    "    vital_smoking = vital_r[['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT', 'SMOKING']].dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT'])\n",
    "    \n",
    "    vital_sys = vital_mean[['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT', 'SYSTOLIC']].dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT'])\n",
    "    vital_dias = vital_mean[['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT', 'DIASTOLIC']].dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT'])\n",
    "    \n",
    "    vital_bmi_p = vital_bmi.groupby(['PATID', 'ENCOUNTERID']).agg({'ORIGINAL_BMI':'last'}).reset_index()\n",
    "    vital_wt_p  =  vital_wt.groupby(['PATID', 'ENCOUNTERID']).agg({'WT':'last'}).reset_index()\n",
    "    vital_ht_p  =  vital_ht.groupby(['PATID', 'ENCOUNTERID']).agg({'HT':'last'}).reset_index()\n",
    "    vital_sys_p = vital_sys.groupby(['PATID', 'ENCOUNTERID'])['SYSTOLIC'].last().reset_index(name = 'SYSTOLIC_MEAN')\n",
    "    vital_dias_p = vital_dias.groupby(['PATID', 'ENCOUNTERID'])['DIASTOLIC'].last().reset_index(name = 'DIASTOLIC_MEAN')\n",
    "    vital_smoking_p  =  vital_smoking.groupby(['PATID', 'ENCOUNTERID']).agg({'SMOKING':'last'}).reset_index()\n",
    "\n",
    "    vital_wt_ht = vital_wt_p.merge(vital_ht_p, on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "    vital_bmi_wt_ht = vital_wt_ht.merge(vital_bmi_p, on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "\n",
    "    calculated_bmi = (vital_bmi_wt_ht['WT'] / (vital_bmi_wt_ht['HT'] ** 2)) * 703\n",
    "\n",
    "    vital_bmi_wt_ht['BMI'] = vital_bmi_wt_ht['ORIGINAL_BMI'].fillna(calculated_bmi)\n",
    "\n",
    "    most_recent_sys = vital_sys.groupby(['PATID', 'ENCOUNTERID']).tail(1).reset_index(drop=True)\n",
    "    most_recent_dias = vital_dias.groupby(['PATID', 'ENCOUNTERID']).tail(1).reset_index(drop=True)\n",
    "\n",
    "    second_most_recent_sys = vital_sys.groupby(['PATID', 'ENCOUNTERID']).nth(-2).reset_index(drop=True)\n",
    "    second_most_recent_dias = vital_dias.groupby(['PATID', 'ENCOUNTERID']).nth(-2).reset_index(drop=True)\n",
    "\n",
    "    sys_slope = most_recent_sys.merge(second_most_recent_sys, on=['PATID', 'ENCOUNTERID'], suffixes=('_recent', '_prev'))\n",
    "    sys_slope['SYSTOLIC_FD'] = (sys_slope['SYSTOLIC_recent'] - sys_slope['SYSTOLIC_prev']) / (sys_slope['DAYS_SINCE_EVENT_recent'] - sys_slope['DAYS_SINCE_EVENT_prev'])\n",
    "    \n",
    "    dias_slope = most_recent_dias.merge(second_most_recent_dias, on=['PATID', 'ENCOUNTERID'], suffixes=('_recent', '_prev'))\n",
    "    dias_slope['DIASTOLIC_FD'] = (dias_slope['DIASTOLIC_recent'] - dias_slope['DIASTOLIC_prev']) / (dias_slope['DAYS_SINCE_EVENT_recent'] - dias_slope['DAYS_SINCE_EVENT_prev'])\n",
    "\n",
    "    vital_bp_fd = pd.merge(sys_slope[['PATID', 'ENCOUNTERID', 'SYSTOLIC_FD']], dias_slope[['PATID', 'ENCOUNTERID', 'DIASTOLIC_FD']], \n",
    "                           on=['PATID', 'ENCOUNTERID'],\n",
    "                           how = 'outer')\n",
    "    vital_t = pd.merge(vital_bmi_wt_ht.drop('ORIGINAL_BMI', axis =1), vital_smoking_p, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    vital_t = pd.merge(vital_t, vital_sys_p, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    vital_t = pd.merge(vital_t, vital_dias_p, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    vital_t = pd.merge(vital_t, vital_bp_fd, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    vital_t['SMOKING_01'] = vital_t['SMOKING'] == '01'\n",
    "    vital_t['SMOKING_02'] = vital_t['SMOKING'] == '02'\n",
    "    vital_t['SMOKING_03'] = vital_t['SMOKING'] == '03'\n",
    "    vital_t['SMOKING_05'] = vital_t['SMOKING'] == '05'\n",
    "    vital_t['SMOKING_06'] = vital_t['SMOKING'] == '06'\n",
    "    vital_t['SMOKING_07'] = vital_t['SMOKING'] == '07'\n",
    "    vital_t.drop('SMOKING', axis = 1, inplace = True)\n",
    "    vital_t.to_pickle(file_paths[1] +'vital'+ '_d' + str(lapsed_day) + '.pkl')\n",
    "    print(f\"Finished processing vital data from site {file_paths[0].split('/')[-2]} for day {lapsed_day}.\", flush = True)\n",
    "    return vital_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d99cadb1-45d6-462c-8ff9-187e69484ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ckd_from_diagnosis(dx):\n",
    "    dx0 = dx.copy()\n",
    "    dx0 = dx0[dx0['DAYS_SINCE_ADMIT'] < 0]\n",
    "    CKD_code = ['585.1','585.2','585.3','585.4','585.5','585.9',\n",
    "                'N18.1','N18.2','N18.3','N18.4','N18.5','N18.6','N18.9']\n",
    "    dx0['PREADM_CKD_FLAG'] = dx0['DX'].isin(CKD_code)\n",
    "    dx0['PREADM_CKD_STAGE'] = 0\n",
    "    dx0.loc[dx0['PREADM_CKD_FLAG'], 'PREADM_CKD_STAGE'] = 1\n",
    "    dx0.loc[dx0['DX'].isin(['585.1', 'N18.1']), 'PREADM_CKD_STAGE'] = 1\n",
    "    dx0.loc[dx0['DX'].isin(['585.2', 'N18.2']), 'PREADM_CKD_STAGE'] = 2\n",
    "    dx0.loc[dx0['DX'].isin(['585.3', 'N18.3']), 'PREADM_CKD_STAGE'] = 3\n",
    "    dx0.loc[dx0['DX'].isin(['585.4', 'N18.4']), 'PREADM_CKD_STAGE'] = 4\n",
    "    dx0.loc[dx0['DX'].isin(['585.5', 'N18.5']), 'PREADM_CKD_STAGE'] = 5\n",
    "    dx0.loc[dx0['DX'].isin(['585.6', 'N18.6']), 'PREADM_CKD_STAGE'] = 6\n",
    "    dx1 = dx0.groupby(['PATID','ENCOUNTERID'])['PREADM_CKD_FLAG'].max().reset_index()\n",
    "    dx2 = dx0.groupby(['PATID','ENCOUNTERID'])['PREADM_CKD_STAGE'].max().reset_index()\n",
    "    ckd_all = dx1.merge(dx2, on =['PATID','ENCOUNTERID'], how = 'left')\n",
    "    return ckd_all.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78b36fd1-8ee3-4418-8215-71afc5a2ee86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dx(file_paths, trim = 0.05, event_type = 'PROGNOSIS'):\n",
    "    output_path = file_paths[1]\n",
    "    aux_path = file_paths[2]\n",
    "    dx, onset = load_and_create_timeline(file_paths, 'AKI_DX', 'DX_DATE', event_type, 999)\n",
    "    \n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].where(dx['DX_TYPE'] != '9', '09')\n",
    "    \n",
    "    # Step 1: Get pre-existing CKD flag and stage\n",
    "    dx_ckd = get_ckd_from_diagnosis(dx) \n",
    "\n",
    "    # Step 2: Get all dx features\n",
    "    icd10toicd09 = pd.read_csv(aux_path + '2018_I10gem.csv', sep=',')\n",
    "    icd10toicd09.columns = ['DX', 'DX09']\n",
    "    dx10 = dx[dx['DX_TYPE'] == '10']\n",
    "    dx10['DX'] = dx10['DX'].str.replace('.', '')\n",
    "    dx10 = pd.merge(dx10, icd10toicd09, on='DX', how='left')\n",
    "    dx10['DX_TYPE'] = np.where(dx10['DX09'].notna(), '09', dx10['DX_TYPE'])\n",
    "    dx10['DX'] = np.where(dx10['DX09'].notna(), dx10['DX09'], dx10['DX'])\n",
    "    dx10 = dx10.drop('DX09', axis=1)\n",
    "    dx = pd.concat([dx[dx['DX_TYPE'] != '10'], dx10], axis=0)\n",
    "    dx['DX'] = dx['DX'].where(dx['DX_TYPE'] != '09', dx['DX'].map(lambda x: x[0:3]))\n",
    "    dx['DX'] = dx['DX'].where(dx['DX_TYPE'] != '10', dx['DX'].map(lambda x: x[0:3]))\n",
    "    dx['DX'] = dx['DX_TYPE'] + '_' + dx['DX']\n",
    "    dx_r = filter_sparse_features(dx, onset, 'DX', trim)\n",
    "    dx_dummies = pd.get_dummies(dx_r[['PATID','ENCOUNTERID', 'DX']], columns=['DX'], prefix='DX')\n",
    "    dx_pivot = dx_dummies.groupby(['PATID', 'ENCOUNTERID']).max().reset_index()\n",
    "    dx_final = dx_pivot.merge(dx_ckd, on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "    dx_final.to_pickle(output_path + 'dx.pkl')\n",
    "    print(f\"Finished processing dx data from {file_paths[0].split('/')[-2]}.\", flush = True)\n",
    "    return dx_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c5cab17-08df-4389-ac05-8afca31d11f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_px(file_paths,  trim = 0.05, event_type = 'PROGNOSIS', lapsed_day = 1):\n",
    "    output_path = file_paths[1]\n",
    "    px, onset = load_and_create_timeline(file_paths, 'AKI_PX', 'PX_DATE', event_type, lapsed_day)\n",
    "    px['PX_TYPE'] = px['PX_TYPE'].where(px['PX_TYPE'] != '9', '09')\n",
    "    px['PX_NUMERIC'] = np.where(px['PX'].str.isnumeric(), px['PX'], 0)\n",
    "    px['PX_NUMERIC'] = px['PX_NUMERIC'].astype(int)\n",
    "    mask_admin = (px['PX_TYPE'] == 'CH') & np.logical_or(np.logical_and(px['PX_NUMERIC'] >= 99202, px['PX_NUMERIC'] <= 99499),\n",
    "                                                         np.logical_and(px['PX_NUMERIC'] >= 80047, px['PX_NUMERIC'] <= 89398))\n",
    "    \n",
    "    px = px[~mask_admin].drop('PX_NUMERIC', axis = 1)\n",
    "    px['PX'] = px['PX_TYPE'] + '_' + px['PX']\n",
    "    px_r = filter_sparse_features(px, onset, 'PX', trim)\n",
    "    px_dummies = pd.get_dummies(px_r[['PATID','ENCOUNTERID', 'PX']], columns=['PX'], prefix='PX') \n",
    "    px_final = px_dummies.groupby(['PATID', 'ENCOUNTERID']).max().reset_index()\n",
    "    px_final.to_pickle(output_path + 'px'+'_d'+str(lapsed_day)+'.pkl')\n",
    "    \n",
    "    print(f\"Finished processing px data from {file_paths[0].split('/')[-2]} for day {lapsed_day}.\", flush = True)\n",
    "    return px_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aa3f75e-f512-4f81-a4a1-6b937428eae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_rxnorm2atc_pmed(med, file_paths):\n",
    "    aux_path = file_paths[2]+'map/'\n",
    "    rxcui2atc = pd.read_parquet(aux_path + 'med_unified_conversion_rx2atc.parquet') \n",
    "    rxcui2atc = rxcui2atc.rename(columns={'RX': 'RXNORM_CUI', \n",
    "                                          'ATC': 'ATC4th'})\n",
    "    rxcui2atc = rxcui2atc[['RXNORM_CUI', 'ATC4th']].dropna().drop_duplicates()\n",
    "    med['RXNORM_CUI'] = med['RXNORM_CUI'].astype('str')\n",
    "    rxcui2atc['RXNORM_CUI'] = rxcui2atc['RXNORM_CUI'].astype('str')\n",
    "    rxcui2atc['ATC4th'] = rxcui2atc['ATC4th'].astype('str')\n",
    "    med = med.merge(rxcui2atc[['RXNORM_CUI', 'ATC4th']], on='RXNORM_CUI', how='left')\n",
    "    med['RX_TYPE'] = 'RXN'\n",
    "    med['RX_CODE'] = med['RXNORM_CUI'].copy()\n",
    "    med['RX_TYPE'] = med['RX_TYPE'].where(med['ATC4th'].isna(), 'ATC')\n",
    "    med['RX_CODE'] = med['RX_CODE'].where(med['ATC4th'].isna(), med['ATC4th'])\n",
    "    med['RX_CODE'] = med['RX_TYPE'] + '_' + med['RX_CODE']\n",
    "    med = med[['PATID', 'ENCOUNTERID', 'RX_TYPE', 'RX_CODE']]\n",
    "    return med\n",
    "\n",
    "def process_pmed(file_paths, trim = 0.05, event_type = 'PROGNOSIS', lapsed_day = 1):\n",
    "    output_path = file_paths[1]\n",
    "    med, onset = load_and_create_timeline(file_paths, 'AKI_PMED', 'RX_START_DATE', event_type, lapsed_day)\n",
    "    med_atc = convert_rxnorm2atc_pmed(med, file_paths)\n",
    "    med_r = filter_sparse_features(med_atc, onset, 'RX_CODE', trim)\n",
    "    med_dummies = pd.get_dummies(med_r[['PATID', 'ENCOUNTERID', 'RX_CODE']], \n",
    "                                 columns=['RX_CODE'], \n",
    "                                 prefix='RX')\n",
    "    med_final = med_dummies.groupby(['PATID', 'ENCOUNTERID']).max().reset_index()\n",
    "    med_final.to_pickle(output_path + 'pmed'+'_d'+str(lapsed_day)+'.pkl')\n",
    "    print(f\"Finished processing pmed data from {file_paths[0].split('/')[-2]} for day {lapsed_day}.\", flush = True)\n",
    "    return med_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15105860-011c-491e-a2fa-e7bcea7119a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_rxnorm2atc(med, file_paths):\n",
    "    aux_path = file_paths[2]+'map/'\n",
    "    rxcui2atc = pd.read_parquet(aux_path+'med_unified_conversion_rx2atc.parquet')  \n",
    "    rxcui2atc = rxcui2atc.rename(columns={'RX': 'MEDADMIN_CODE', 'ATC': 'ATC4th'})\n",
    "    rxcui2atc = rxcui2atc[['MEDADMIN_CODE', 'ATC4th']].dropna().drop_duplicates()\n",
    "    med['MEDADMIN_CODE'] = med['MEDADMIN_CODE'].astype('str')\n",
    "    rxcui2atc['MEDADMIN_CODE'] = rxcui2atc['MEDADMIN_CODE'].astype('str')\n",
    "    rxcui2atc['ATC4th'] = rxcui2atc['ATC4th'].astype('str')\n",
    "    med = med.merge(rxcui2atc[['MEDADMIN_CODE', 'ATC4th']], on='MEDADMIN_CODE', how='left')\n",
    "    med['MEDADMIN_TYPE'] = 'RXN'\n",
    "    med['MEDADMIN_TYPE'] = med['MEDADMIN_TYPE'].where(med['ATC4th'].isna(), 'ATC')\n",
    "    med['MEDADMIN_CODE'] = med['MEDADMIN_CODE'].where(med['ATC4th'].isna(), med['ATC4th'])\n",
    "    med['MEDADMIN_CODE'] = med['MEDADMIN_TYPE'] + '_' + med['MEDADMIN_CODE']\n",
    "    med = med[['PATID', 'ENCOUNTERID', 'MEDADMIN_TYPE', 'MEDADMIN_CODE']]\n",
    "    return med\n",
    "\n",
    "def convert_ndc2atc(med, file_paths):\n",
    "    aux_path = file_paths[2]+'map/'\n",
    "    ndc2rx = pd.read_parquet(aux_path+'med_unified_conversion_nd2rx.parquet')  \n",
    "    rx2atc = pd.read_parquet(aux_path+'med_unified_conversion_rx2atc.parquet')\n",
    "    ndc2atc = ndc2rx.merge(rx2atc, on = 'RX', how = 'inner')\n",
    "    ndc2atc = ndc2atc.rename(columns={'ND': 'MEDADMIN_CODE', 'ATC': 'ATC4th'})\n",
    "    ndc2atc = ndc2atc[['MEDADMIN_CODE', 'ATC4th']].dropna().drop_duplicates()\n",
    "    med['MEDADMIN_CODE'] = med['MEDADMIN_CODE'].astype('str')\n",
    "    ndc2atc['MEDADMIN_CODE'] = ndc2atc['MEDADMIN_CODE'].astype('str')\n",
    "    ndc2atc['ATC4th'] = ndc2atc['ATC4th'].astype('str')\n",
    "    med = med.merge(ndc2atc[['MEDADMIN_CODE', 'ATC4th']], on='MEDADMIN_CODE', how='left')\n",
    "    med['MEDADMIN_TYPE'] = 'ND'\n",
    "    med['MEDADMIN_TYPE'] = med['MEDADMIN_TYPE'].where(med['ATC4th'].isna(), 'ATC')\n",
    "    med['MEDADMIN_CODE'] = med['MEDADMIN_CODE'].where(med['ATC4th'].isna(), med['ATC4th'])\n",
    "    med['MEDADMIN_CODE'] = med['MEDADMIN_TYPE'] + '_' + med['MEDADMIN_CODE']\n",
    "    med = med[['PATID', 'ENCOUNTERID', 'MEDADMIN_TYPE', 'MEDADMIN_CODE']]\n",
    "    return med\n",
    "\n",
    "def process_amed(file_paths, trim = 0.05, event_type = 'PROGNOSIS', lapsed_day = 1):\n",
    "    output_path = file_paths[1]\n",
    "    med, onset = load_and_create_timeline(file_paths, 'AKI_AMED', 'MEDADMIN_START_DATE', event_type, lapsed_day)\n",
    "\n",
    "    med_rx = med[med['MEDADMIN_TYPE'] == 'RX'][['PATID', 'ENCOUNTERID', 'MEDADMIN_CODE']]\n",
    "    med_nd = med[med['MEDADMIN_TYPE'] == 'ND'][['PATID', 'ENCOUNTERID', 'MEDADMIN_CODE']]\n",
    "\n",
    "    if not med_rx.empty:\n",
    "        med_rx = convert_rxnorm2atc(med_rx, file_paths)\n",
    "\n",
    "    if not med_nd.empty:\n",
    "        med_nd = convert_ndc2atc(med_nd, file_paths)\n",
    "\n",
    "    med_atc = pd.concat([med_rx, med_nd], axis=0, ignore_index=True)\n",
    "    med_r = filter_sparse_features(med_atc, onset, 'MEDADMIN_CODE', trim)\n",
    "\n",
    "    med_dummies = pd.get_dummies(med_r[['PATID', 'ENCOUNTERID', 'MEDADMIN_CODE']], \n",
    "                                 columns=['MEDADMIN_CODE'], \n",
    "                                 prefix='RX')\n",
    "    med_final = med_dummies.groupby(['PATID', 'ENCOUNTERID']).max().reset_index()\n",
    "    med_final.to_pickle(output_path + 'amed'+'_d'+str(lapsed_day)+'.pkl')\n",
    "    print(f\"Finished processing amed data from {file_paths[0].split('/')[-2]} for day {lapsed_day}.\", flush = True)\n",
    "    return med_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b16f5779-d543-407f-bc6e-b9ae619468e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lab_scr(file_paths, event_type, lapsed_day):\n",
    "    scr, onset = load_and_create_timeline(file_paths, 'AKI_LAB_SCR', 'SPECIMEN_DATE', event_type, lapsed_day)\n",
    "    scr = replace_outliers(scr, None, 'RESULT_NUM')\n",
    "\n",
    "    scr_mean = scr[['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT', 'RESULT_NUM']].groupby(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT'])['RESULT_NUM'].mean().reset_index(name='SCR_MEAN')\n",
    "    scr_mean_sorted = scr_mean.dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT'])\n",
    "    scr_mean_mrv = scr_mean_sorted.groupby(['PATID', 'ENCOUNTERID']).agg({'SCR_MEAN': 'last'}).reset_index()\n",
    "\n",
    "    scr_mean_sorted['ROWID'] = range(1, len(scr_mean_sorted) + 1)\n",
    "    most_recent_scr = scr_mean_sorted.groupby(['PATID', 'ENCOUNTERID']).agg({'SCR_MEAN': 'last', 'DAYS_SINCE_EVENT': 'last', 'ROWID': 'last'}).reset_index()\n",
    "    mrv_scr = most_recent_scr['ROWID'].unique()\n",
    "    most_recent_scr = most_recent_scr[['PATID', 'ENCOUNTERID', 'SCR_MEAN', 'DAYS_SINCE_EVENT']].rename(columns={'SCR_MEAN': 'MOST_RECENT_VALUE', 'DAYS_SINCE_EVENT': 'MOST_RECENT_DAY'})\n",
    "\n",
    "    second_most_recent_scr = scr_mean_sorted[~scr_mean_sorted['ROWID'].isin(mrv_scr)].groupby(['PATID', 'ENCOUNTERID']).agg({'SCR_MEAN': 'last', 'DAYS_SINCE_EVENT': 'last'}).reset_index()\n",
    "    second_most_recent_scr = second_most_recent_scr.rename(columns={'SCR_MEAN': 'SECOND_MOST_RECENT_VALUE', 'DAYS_SINCE_EVENT': 'SECOND_MOST_RECENT_DAY'})\n",
    "\n",
    "    scr_diff = pd.merge(most_recent_scr, second_most_recent_scr, on=['PATID', 'ENCOUNTERID'], how='inner')\n",
    "    scr_diff['DAYS_DIFF'] = scr_diff['MOST_RECENT_DAY'] - scr_diff['SECOND_MOST_RECENT_DAY']\n",
    "    scr_diff['SCR_CHANGE'] = scr_diff['MOST_RECENT_VALUE'] - scr_diff['SECOND_MOST_RECENT_VALUE']\n",
    "    scr_diff['SCR_FD'] = scr_diff['SCR_CHANGE'] / scr_diff['DAYS_DIFF']\n",
    "\n",
    "    scr_final = scr_mean_mrv[['PATID', 'ENCOUNTERID', 'SCR_MEAN']].merge(scr_diff[['PATID', 'ENCOUNTERID', 'SCR_FD']], on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "\n",
    "    scr_final.to_pickle(file_paths[1] + 'scr' + '_d' + str(lapsed_day) + '.pkl')\n",
    "    print(f\"Finished processing lab scr data from {file_paths[0].split('/')[-2]} for day {lapsed_day}.\", flush=True)\n",
    "    return scr_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ec61fac-7598-4f61-96f8-77f1c781b9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unify_lab_units(file_paths, event_type = 'PROGNOSIS'):\n",
    "    aux_path = file_paths[2]+'map/'\n",
    "    output_path = file_paths[1]\n",
    "    site_name = file_paths[0].split('/')[-2]\n",
    "    labtest, onset = load_and_create_timeline(file_paths, 'AKI_LAB', 'SPECIMEN_DATE', event_type, 8)\n",
    "    labtest['site']= site_name\n",
    "    \n",
    "    UCUMunitX = pd.read_csv(aux_path + 'UCUMunitX.csv')\n",
    "    UCUMunitX.factor_final = UCUMunitX.factor_final.fillna(1)\n",
    "    UCUMqualX = pd.read_csv(aux_path + 'UCUMqualX.csv')\n",
    "    local_custom_convert = pd.read_csv(aux_path + 'local_custom_convert.csv')\n",
    "    loincmap3 = pd.read_csv(aux_path + 'GroupLoincTerms.csv')\n",
    "\n",
    "    labtest2 = labtest.merge(local_custom_convert, on = ['LAB_LOINC', 'site'], how='left')\n",
    "    labtest2['NEW_UNIT'] = np.where(labtest2['TARGET_UNIT'].notna(), labtest2['TARGET_UNIT'], labtest2['RESULT_UNIT'])\n",
    "    labtest2['NEW_RESULT_NUM'] = np.where(labtest2['TARGET_UNIT'].notna(), labtest2['Multipliyer']*labtest2['RESULT_NUM'], labtest2['RESULT_NUM'])\n",
    "\n",
    "    labtest3 = labtest2.copy()\n",
    "    labtest3['RESULT_UNIT'] = labtest3['NEW_UNIT']\n",
    "    labtest3['RESULT_NUM'] = labtest3['NEW_RESULT_NUM']\n",
    "    labtest3 = labtest3.drop(['NEW_UNIT', 'NEW_RESULT_NUM', 'SOURCE_UNIT', 'TARGET_UNIT', 'LONG_COMMON_NAME', 'Multipliyer'], axis=1)\n",
    "    labtest4 = labtest3.merge(UCUMunitX, on = ['LAB_LOINC', 'RESULT_UNIT'], how='left').copy()\n",
    "\n",
    "    filter1 = labtest4['FINAL_UNIT'].notna() \n",
    "\n",
    "    labtest4['NEW_UNIT'] = np.where(filter1, labtest4['FINAL_UNIT'], labtest4['RESULT_UNIT'])\n",
    "    labtest4['NEW_RESULT_NUM'] = np.where(filter1, labtest4['factor_final']*labtest4['RESULT_NUM'], labtest4['RESULT_NUM'])\n",
    "    labtest4['NEW_LAB_LOINC'] = np.where(filter1, labtest4['GroupId'], labtest4['LAB_LOINC'])\n",
    "    labtest4['RESULT_UNIT'] = labtest4['NEW_UNIT']\n",
    "    labtest4['RESULT_NUM'] = labtest4['NEW_RESULT_NUM']\n",
    "    labtest4['LAB_LOINC'] = labtest4['NEW_LAB_LOINC']\n",
    "    labtest4 = labtest4.drop(['GroupId', 'EXAMPLE_UCUM_UNITS',\n",
    "                              'EXAMPLE_UCUM_UNITS_FINAL', 'RESULT_UNIT_CONSENSUS', 'FINAL_UNIT',\n",
    "                              'FINAL_Multiplyer', 'RESULT_UNIT_API', 'FINAL_UNIT_API', 'factor_final',\n",
    "                              'NEW_UNIT', 'NEW_RESULT_NUM', 'NEW_LAB_LOINC'], axis=1)\n",
    "    \n",
    "    labtest5 = labtest4.copy()\n",
    "    labtest5 = labtest5.merge(UCUMqualX[['LAB_LOINC', 'GroupId']].drop_duplicates(), on='LAB_LOINC', how='left')\n",
    "    filter2 = labtest5['GroupId'].notna() #& labtest5['RESULT_NUM'].isna()\n",
    "    labtest5['NEW_LAB_LOINC'] = np.where(filter2, labtest5['GroupId'], labtest5['LAB_LOINC'])\n",
    "    labtest5['LAB_LOINC'] = labtest5['NEW_LAB_LOINC']\n",
    "    labtest5 = labtest5.drop(['GroupId','NEW_LAB_LOINC'],axis=1)\n",
    "    labtest5 = labtest5.drop('site',axis=1)\n",
    "    labtest5 = labtest5.drop_duplicates()\n",
    "\n",
    "    labtest5.to_pickle(output_path + 'lab_unified.pkl')\n",
    "    print(f\"Finished unifying lab units from {file_paths[0].split('/')[-2]}.\", flush = True)\n",
    "    return labtest5\n",
    "\n",
    "def manual_update_lab_units(file_paths):\n",
    "    aux_path = file_paths[2]+'map/'\n",
    "    output_path = file_paths[1]\n",
    "    lab_test = pd.read_pickle(output_path + 'lab_unified.pkl')\n",
    "    group_convert_tbl = pd.read_csv(aux_path + 'group_conversion_custom.csv')\n",
    "    group_convert_tbl.replace('np.nan', np.nan, inplace=True)\n",
    "    lab_test2 = lab_test.merge(group_convert_tbl[group_convert_tbl['site'] == site_name], \n",
    "                               left_on = ['LAB_LOINC', 'RESULT_UNIT'], \n",
    "                               right_on = ['LAB_LOINC', 'SOURCE_UNIT'], \n",
    "                               how = 'left')\n",
    "    lab_test2.loc[lab_test2['GroupId'].notna(), 'LAB_LOINC'] = lab_test2.loc[lab_test2['GroupId'].notna(), 'GroupId']\n",
    "    lab_test2.loc[lab_test2['GroupId'].notna(), 'RESULT_UNIT'] = lab_test2.loc[lab_test2['GroupId'].notna(), 'TARGET_UNIT']\n",
    "    lab_test2.loc[lab_test2['GroupId'].notna(), 'RESULT_NUM'] = (lab_test2.loc[lab_test2['GroupId'].notna(), 'RESULT_NUM'] * \n",
    "                                                                 lab_test2.loc[lab_test2['GroupId'].notna(), 'factor'])\n",
    "    lab_test2.to_pickle(output_path + 'lab_unified_updated.pkl')\n",
    "    return lab_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fbe0efc-1749-4356-8d67-272bdd7e8863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_numeric_lab(file_paths, trim = 0.05, lapsed_day = 1):\n",
    "    output_path = file_paths[1]\n",
    "    \n",
    "    onset = pd.read_pickle(output_path + 'onset.pkl')\n",
    "    lab = pd.read_pickle(output_path +'lab_unified_updated.pkl')\n",
    "    \n",
    "    lab = lab[lab['DAYS_SINCE_EVENT'] < lapsed_day]\n",
    "    lab = lab[(lab['LAB_LOINC'].notna()) & (lab['RESULT_NUM'].notna())]\n",
    "    lab = replace_outliers(lab, 'LAB_LOINC', 'RESULT_NUM')\n",
    "    lab = filter_sparse_features(lab, onset, 'LAB_LOINC', trim)\n",
    "\n",
    "    lab_r = lab[['LAB_LOINC','PATID', 'ENCOUNTERID', 'RESULT_NUM', 'DAYS_SINCE_EVENT']].groupby(['LAB_LOINC', 'PATID', 'ENCOUNTERID', 'DAYS_SINCE_EVENT']).mean().reset_index()\n",
    "    lab_f = lab_r.sort_values(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_EVENT']).groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC']).agg({'RESULT_NUM':'last'}).reset_index()\n",
    "    df_l = lab_f.pivot(index=['PATID', 'ENCOUNTERID'], columns='LAB_LOINC', values='RESULT_NUM').reset_index()\n",
    "    df_l.columns = ['LAB_' + col if col not in ['PATID','ENCOUNTERID'] else col for col in df_l.columns]\n",
    "    df_lab = df_l.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "\n",
    "    df_lab.to_pickle(output_path + 'lab_num' + '_d' + str(lapsed_day) +'.pkl')\n",
    "    print(f\"Finished processing numeric lab tests from {file_paths[0].split('/')[-2]} for day {lapsed_day}.\", flush = True)\n",
    "    return df_lab\n",
    "\n",
    "\n",
    "def process_categorical_lab(file_paths, trim = 0.05, lapsed_day = 1):\n",
    "    output_path = file_paths[1]\n",
    "    onset = pd.read_pickle(output_path + 'onset.pkl')\n",
    "    lab = pd.read_pickle(output_path +'lab_unified_updated.pkl')\n",
    "    lab = lab[lab['DAYS_SINCE_EVENT'] < lapsed_day]\n",
    "    lab = lab[(lab['LAB_LOINC'].notna()) & (lab['RESULT_QUAL'].notna()) &  ~(lab['RESULT_QUAL'].str.contains('OT', case=False)\n",
    "                                                                             | lab['RESULT_QUAL'].str.contains('NI', case=False)\n",
    "                                                                             | lab['RESULT_QUAL'].str.contains('INVALID', case=False))]\n",
    "\n",
    "    lab = filter_sparse_features(lab, onset, 'LAB_LOINC', trim)\n",
    "\n",
    "    labcat = lab[['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_EVENT', 'RESULT_QUAL']]\n",
    "    \n",
    "    if labcat.empty:\n",
    "        labcat[['PATID', 'ENCOUNTERID']].to_pickle(output_path + 'lab_qual' + '_d' + str(lapsed_day) +'.pkl')\n",
    "        return pd.DataFrame(columns=['PATID', 'ENCOUNTERID'])\n",
    "    \n",
    "    lab_mode = labcat.groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_EVENT']).agg(pd.Series.mode).reset_index()\n",
    "    lab_mode_nnd = lab_mode.loc[lab_mode['RESULT_QUAL'].apply(type) == str].copy()\n",
    "    lab_mode_nd = lab_mode.loc[lab_mode['RESULT_QUAL'].apply(type) != str].copy()\n",
    "    pattern = '[\\[\\]\\']'\n",
    "    lab_mode_nd['RESULT_QUAL'] = lab_mode_nd['RESULT_QUAL'].apply(lambda x: re.sub(pattern, \"\", np.array2string(x, separator='-')))\n",
    "    lab_mode = pd.concat([lab_mode_nd, lab_mode_nnd], ignore_index=True)\n",
    "\n",
    "    labcat_t = lab_mode.sort_values(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_EVENT']).groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC']).agg({'RESULT_QUAL': 'last'}).reset_index()\n",
    "\n",
    "    labcat_t['LAB_LOINC'] = 'LABCAT_' + labcat_t['LAB_LOINC'] + \"(\" + labcat_t['RESULT_QUAL'] + \")\"\n",
    "    labcat_t['dummy'] = True\n",
    "    labcat_t = labcat_t[['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'dummy']]\n",
    "    labcat_t = labcat_t.pivot_table(index=['PATID', 'ENCOUNTERID'],\n",
    "                                    columns='LAB_LOINC',\n",
    "                                    values='dummy',\n",
    "                                    fill_value=False).reset_index()\n",
    "\n",
    "    labcat_t.to_pickle(output_path + 'lab_qual' + '_d' + str(lapsed_day) +'.pkl')\n",
    "    print(f\"Finished processing categorical lab tests from {file_paths[0].split('/')[-2]} for day {lapsed_day}.\", flush=True)\n",
    "    return labcat_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fa3bec5-8703-49f7-baa4-84f9fe6264df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_feature_data(file_paths, lapsed_day):\n",
    "    input_path = file_paths[0]\n",
    "    output_path = file_paths[1]\n",
    "    site_name = file_paths[0].split('/')[-2]\n",
    "    onset = pd.read_pickle(output_path+'onset.pkl')\n",
    "    covid = pd.read_pickle(input_path+'/AKI_ONSETS_2'+'.pkl')\n",
    "    onset = onset.merge(covid[['ENCOUNTERID', 'PATID', 'BCCOVID']], on = ['ENCOUNTERID', 'PATID'], how = 'left')\n",
    "    demo = pd.read_pickle(output_path+'demo.pkl')\n",
    "    dx = pd.read_pickle(output_path+'dx.pkl')\n",
    "    px = pd.read_pickle(output_path+'px_'+ 'd'+str(lapsed_day) + '.pkl')\n",
    "    rx = pd.read_pickle(output_path+'amed_'+ 'd'+str(lapsed_day) + '.pkl') if site_name not in ['UTSW', 'UofU' ,'UPITT','UNMC'] else pd.read_pickle(output_path+'pmed_'+ 'd'+str(lapsed_day) + '.pkl')\n",
    "    \n",
    "    scr = pd.read_pickle(output_path+'scr_'+ 'd'+str(lapsed_day) + '.pkl')\n",
    "    vital = pd.read_pickle(output_path+'vital_'+ 'd'+str(lapsed_day) + '.pkl')\n",
    "    labnum = pd.read_pickle(output_path+'lab_num_'+ 'd'+str(lapsed_day) + '.pkl')\n",
    "    labcat= pd.read_pickle(output_path+'lab_qual_'+ 'd'+str(lapsed_day) + '.pkl')\n",
    "    \n",
    "    if not labcat.empty:\n",
    "        labcat_mean = labcat.drop(['PATID', 'ENCOUNTERID'], axis = 1).sum()/onset.shape[0]\n",
    "        labcat = labcat.drop(list((labcat_mean[labcat_mean < 0.05]).index), axis =1)\n",
    "    \n",
    "    df1 = onset.merge(demo, on = ['PATID', 'ENCOUNTERID'], how = 'left')\n",
    "    df2 = df1.merge(dx, on = ['PATID', 'ENCOUNTERID'], how = 'left')\n",
    "    df3 = df2.merge(px, on= ['PATID', 'ENCOUNTERID'], how='left')\n",
    "    df4 = df3.merge(rx, on = ['PATID', 'ENCOUNTERID'], how = 'left')\n",
    "    df5 = df4.merge(scr, on = ['PATID', 'ENCOUNTERID'], how = 'left')\n",
    "    df6 = df5.merge(vital, on = ['PATID', 'ENCOUNTERID'], how = 'left')\n",
    "    df7 = df6.merge(labnum, on = ['PATID', 'ENCOUNTERID'], how = 'left')\n",
    "    data = df7.merge(labcat, on = ['PATID', 'ENCOUNTERID'], how = 'left')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_feature_data(file_paths, impute = False, lapsed_day = 1):\n",
    "    data = merge_feature_data(file_paths, lapsed_day)\n",
    "    data.replace('NaN', np.nan, inplace=True)\n",
    "    \n",
    "    bool_col = [col for col in data.columns if col.startswith('RX_') \n",
    "                                            or col.startswith('PX_') \n",
    "                                            or col.startswith('DX_')\n",
    "                                            or col.startswith('LABCAT_')\n",
    "                                            or col.startswith('PREADM_CKD_FLAG')\n",
    "                                            or col.startswith('SMOKING_')]\n",
    "    data[bool_col] = data[bool_col].fillna(False)\n",
    "    \n",
    "    for col in bool_col:\n",
    "        data[col] = data[col].astype(bool)\n",
    "    \n",
    "    fl_col = [col for col in data.columns \n",
    "                           if col.startswith(\"LAB_\")\n",
    "                           or col.startswith(\"SCR_\")\n",
    "                           or col.startswith(\"SYSTOLIC_\") \n",
    "                           or col.startswith(\"DIASTOLIC_\") \n",
    "                           or col.startswith(\"BMI\") \n",
    "                           or col.startswith(\"WT\") \n",
    "                           or col.startswith(\"HT\")]\n",
    "    \n",
    "    if impute:\n",
    "        for col in fl_col:\n",
    "            data[col].fillna(data[col].mean(), inplace=True)\n",
    "                  \n",
    "    for col in fl_col:\n",
    "        data[col] = data[col].astype(float)\n",
    "        \n",
    "    site_name = file_paths[0].split('/')[-2]\n",
    "    if site_name == 'UTHSCSA':\n",
    "            data['LAB_5902-2'] = data['LAB_5902-2'] * 10\n",
    "\n",
    "    loincmap3 = pd.read_csv(os.path.join(file_paths[2], 'map/GroupLoincTerms.csv'))\n",
    "    mmc = loincmap3[loincmap3['Category']=='Mass-Molar conversion'][['GroupId']]    \n",
    "    contains_group_id = data.columns[data.columns.to_series().apply(lambda col: any(mmc['GroupId'].apply(lambda x: x in col)))]\n",
    "\n",
    "    data.drop(columns=contains_group_id).to_pickle(file_paths[1] + 'data_'+ 'd'+str(lapsed_day) + '.pkl')\n",
    "    print(f\"Finish processing feature data from {site_name} for day {lapsed_day}.\", flush = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e20b6a-8d23-45e2-9f29-cac02486ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './'\n",
    "site_list = ['KUMC', 'UMHC', 'UNMC', 'UTHSCSA']\n",
    "event_type = 'PROGNOSIS'\n",
    "trim = 0.05\n",
    "impute = False\n",
    "\n",
    "logging.basicConfig(filename= base_path+'log/processing_errors.log', level=logging.ERROR)\n",
    "\n",
    "for site_name in site_list:\n",
    "    try:\n",
    "        file_paths = get_data_file_path(base_path, site_name)\n",
    "        process_onset_and_recovery(file_paths, aggfunc_7d = 'last', aggfunc_1y = 'mean', keep_ckd = False)\n",
    "        process_demo(file_paths)\n",
    "        process_dx(file_paths, trim, event_type)\n",
    "        unify_lab_units(file_paths, event_type)\n",
    "        manual_update_lab_units(file_paths)\n",
    "\n",
    "        for lapsed_day in range(1,8):\n",
    "                process_vital(file_paths, event_type, lapsed_day)\n",
    "                process_px(file_paths, trim, event_type, lapsed_day)\n",
    "                process_amed(file_paths, trim, event_type, lapsed_day)\n",
    "                process_pmed(file_paths, trim, event_type, lapsed_day)\n",
    "                process_lab_scr(file_paths, event_type, lapsed_day)\n",
    "                process_numeric_lab(file_paths, trim, lapsed_day)\n",
    "                process_categorical_lab(file_paths, trim, lapsed_day)\n",
    "                process_feature_data(file_paths, impute, lapsed_day)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing site {site_name}: {e}\")\n",
    "        print(f\"Error processing site {site_name}. Check log for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187d8a8-f6cf-4e4e-8220-532c927554e0",
   "metadata": {},
   "source": [
    "#### Step 4: Prepare Multi-state Transition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48fbcd-ab25-4d52-a90a-f6c6ca17aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ft_list(aki_subgrp):\n",
    "    if aki_subgrp == 'aki1':\n",
    "        fts_dict = {\n",
    "                 'SCR_BASELINE' : 'Baseline SCr (mg/dL)', \n",
    "                 'SCR_FD' : 'Most recent SCr (slope, mg/(dL*day))',\n",
    "                 'SCR_MEAN' : 'Most recent SCr (level, mg/dL)',\n",
    "                 'SYSTOLIC_FD': 'Systolic pressure (slope, mmHg/day)',\n",
    "                 'SYSTOLIC_MEAN': 'Systolic pressure (level, mmHg)',\n",
    "                 'DIASTOLIC_FD': 'Diastolic pressure (slope, mmHg/day)', \n",
    "                 'DIASTOLIC_MEAN': 'Diastolic pressure (level, mmHg)', \n",
    "                  'BMI': 'BMI', \n",
    "                  'DX_09_401': 'Essential Hypertension',\n",
    "                  'ONSET_SINCE_ADMIT': 'Days since admission',\n",
    "                  'LAB_LG13614-9': 'Anion Gap (mEq/L)',\n",
    "                  'LAB_LG5465-2': 'Albumin (g/dL)',\n",
    "                  'LAB_LG5665-7': 'ALP (IU/L)',\n",
    "                  'LAB_LG6033-7': 'AST (IU/L)',\n",
    "                  'LAB_LG6199-6': 'Bilirubin (mg/dL)',\n",
    "                  'LAB_LG12080-4': 'BNP (pg/mL)',\n",
    "                  'LAB_LG7247-2':  'Calcium (mg/dL)',\n",
    "                  'LAB_LG4454-7':  'CO2 (mEq/L)',\n",
    "                  'LAB_LG7967-5':  'Glucose (mg/dL)',   \n",
    "                  'LAB_LG6039-4': 'Lactate (mmol/L)',\n",
    "                  'LAB_736-9': 'Lymphocytes/(100*Leukocytes)',\n",
    "                  'LAB_LG10990-6': 'Potassium (mEq/L)',\n",
    "                 'LAB_5902-2':  'PT (s)'\n",
    "                 } \n",
    "\n",
    "        imp_ft_lst = list(fts_dict.keys())\n",
    "    else:\n",
    "        fts_dict = {\n",
    "                     'SCR_BASELINE' : 'Baseline SCr (mg/dL)', \n",
    "                     'SCR_FD' : 'Most recent SCr (slope, mg/(dL*day))',\n",
    "                     'SCR_MEAN' : 'Most recent SCr (level, mg/dL)',\n",
    "                     'SYSTOLIC_FD': 'Systolic pressure (slope, mmHg/day)',\n",
    "                     'SYSTOLIC_MEAN': 'Systolic pressure (level, mmHg)',\n",
    "                     'DIASTOLIC_FD': 'Diastolic pressure (slope, mmHg/day)',  \n",
    "                     'DIASTOLIC_MEAN': 'Diastolic pressure (level, mmHg)',\n",
    "                      'AGE': 'Age',\n",
    "                      'ONSET_SINCE_ADMIT': 'Days since admission',\n",
    "                      'LAB_LG5465-2': 'Albumin (g/dL)',\n",
    "                      'LAB_LG5665-7': 'ALP (IU/L)',\n",
    "                      'LAB_LG6033-7': 'AST (IU/L)',\n",
    "                      'LAB_LG6199-6': 'Bilirubin (mg/dL)',\n",
    "                      'LAB_LG12080-4': 'BNP (pg/mL)',\n",
    "                      'LAB_LG7247-2':  'Calcium (mg/dL)',\n",
    "                      'LAB_LG4454-7':  'CO2 (mEq/L)',\n",
    "                      'LAB_4544-3': 'Hematocrit (%)',  \n",
    "                      'LAB_LG10990-6': 'Potassium (mEq/L)',\n",
    "                      'LAB_LG32850-6': 'RBC (10^6 cells/µL)'\n",
    "                     }  \n",
    "\n",
    "        imp_ft_lst = list(fts_dict.keys())\n",
    "    return imp_ft_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00523e-d9c0-4c5f-a6cb-308c28102c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_MICE(base_path, site_name, ft_list):\n",
    "    file_paths = get_data_file_path(base_path, site_name)\n",
    "\n",
    "    for t in range(1, 8):\n",
    "        df = pd.read_pickle(file_paths[1] + 'data_'+ 'd'+str(t) + '.pkl')\n",
    "\n",
    "        # Select only the relevant columns for imputation\n",
    "        cols_to_impute = [col for col in ft_list if col in df.columns]\n",
    "\n",
    "        df_subset = df[cols_to_impute]  \n",
    "\n",
    "        imputer = IterativeImputer(max_iter=50, \n",
    "                                   random_state=42)\n",
    "\n",
    "        df_imputed = pd.DataFrame(imputer.fit_transform(df_subset), columns=cols_to_impute)\n",
    "\n",
    "        df_imputed['ENCOUNTERID'] = df['ENCOUNTERID'].values\n",
    "\n",
    "        # Ensure all variables in imp_ft_lst are present in data, filling missing ones with NaN\n",
    "        for var in ft_list:\n",
    "            if var not in df.columns:\n",
    "                df_imputed[var] = np.nan\n",
    "\n",
    "        df_imputed.to_pickle(file_paths[1] + 'data_mice_'+ 'd'+str(t) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459d34d-4cb6-4b20-bba1-974ef2aabe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_data(base_path, site_name, imp_ft_lst, aki_subgrp): \n",
    "    file_paths = get_data_file_path(base_path, site_name)\n",
    "    onset0 = pd.read_pickle(file_paths[1]+'onset.pkl')\n",
    "    demo = pd.read_pickle(file_paths[0] + 'AKI_DEMO'+'.pkl') \n",
    "    demo_deduplicated = demo[['PATID', 'ENCOUNTERID', 'DEATH_DATE']].drop_duplicates()\n",
    "\n",
    "    demo_cleaned = (demo_deduplicated\n",
    "                    .groupby(['PATID'], as_index=False)\n",
    "                    .agg({'DEATH_DATE': lambda x: x.max() if x.notna().any() else pd.NaT}))\n",
    "\n",
    "\n",
    "    onset = onset0.merge(demo_cleaned[['PATID',  'DEATH_DATE']], \n",
    "                         on = 'PATID', \n",
    "                         how = 'left')\n",
    "\n",
    "    onset['DEATH_SINCE_ONSET'] = (onset['DEATH_DATE'] - onset['ONSET_DATE']).dt.days\n",
    "    onset['DEATH_SINCE_ONSET'] = onset['DEATH_SINCE_ONSET'].fillna(1000000)\n",
    "    onset['DEATH_SINCE_ONSET'] = onset['DEATH_SINCE_ONSET'].astype(int)\n",
    "    onset.loc[onset['DEATH_SINCE_ONSET'] < 0, 'DEATH_SINCE_ONSET'] = 1000000\n",
    "    \n",
    "    if aki_subgrp == 'aki1':\n",
    "        onset = onset[onset['AKI_INIT_STG'] == 1]\n",
    "    elif aki_subgrp == 'aki23':\n",
    "        onset = onset[onset['AKI_INIT_STG'] >1]\n",
    "    \n",
    "    df_scr = load_and_filter_scr(onset, file_paths)\n",
    "    df_scr0, df_admit = load_onset_data(file_paths)\n",
    "    df_rrt = get_rrt(df_admit, file_paths)\n",
    "    df_scr_filtered = df_scr[df_scr['DAYS_SINCE_ONSET'] >= 0]\n",
    "    df_scr_filtered = df_scr_filtered.merge(df_rrt[['PATID', 'ENCOUNTERID', 'RRT_ONSET_DATE']], \n",
    "                                            on =['PATID', 'ENCOUNTERID'], \n",
    "                                            how = 'left')\n",
    "    df_scr_filtered['RRT_SINCE_ONSET'] = (df_scr_filtered['RRT_ONSET_DATE'] - df_scr_filtered['ONSET_DATE']).dt.days\n",
    "\n",
    "    df_scr_filtered['AKI_STG_TMP'] = 1\n",
    "    mask_noaki = (df_scr_filtered['RESULT_NUM'] < 1.5 * df_scr_filtered['SCR_BASELINE']) & (df_scr_filtered['RESULT_NUM'] < (0.3 + df_scr_filtered['SCR_REFERENCE']))\n",
    "    mask_aki2 = (df_scr_filtered['RESULT_NUM'] >= 2.0 * df_scr_filtered['SCR_BASELINE']) & (df_scr_filtered['RESULT_NUM'] < 3.0 * df_scr_filtered['SCR_BASELINE'])\n",
    "    mask_aki3 = (\n",
    "                 (df_scr_filtered['RESULT_NUM']>=3.0 * df_scr_filtered['SCR_BASELINE']) | (df_scr_filtered['RESULT_NUM']>=4)  # scr criteria\n",
    "                 | ((df_scr_filtered['RRT_SINCE_ONSET'] <= df_scr_filtered['DAYS_SINCE_ONSET']) & (df_scr_filtered['RRT_SINCE_ONSET'] >= 0)) # rrt criteria\n",
    "                )\n",
    "    df_scr_filtered.loc[mask_noaki,'AKI_STG_TMP'] = 0\n",
    "    df_scr_filtered.loc[mask_aki2,'AKI_STG_TMP'] = 2\n",
    "    df_scr_filtered.loc[mask_aki3,'AKI_STG_TMP'] = 3\n",
    "    \n",
    "    df_scr_filtered['AKI_STATUS_CHANGE'] = np.nan\n",
    "    df_scr_filtered.loc[df_scr_filtered['AKI_STG_TMP'] == 0,'AKI_STATUS_CHANGE'] = 1\n",
    "    df_scr_filtered.loc[df_scr_filtered['AKI_STG_TMP'] == 1,'AKI_STATUS_CHANGE'] = 2\n",
    "    df_scr_filtered.loc[df_scr_filtered['AKI_STG_TMP'] > 1,'AKI_STATUS_CHANGE'] = 3\n",
    "\n",
    "    df_status = df_scr_filtered[(df_scr_filtered['DAYS_SINCE_ONSET'] <=7)][['PATID','ENCOUNTERID','AKI_INIT_STG', 'AKI_STG_TMP', 'AKI_STATUS_CHANGE', 'DAYS_SINCE_ONSET']]\n",
    "\n",
    "    onset_filtered = onset[(onset['DISCHARGE_SINCE_ONSET'] >= 0)]\n",
    "    \n",
    "    df_msm = df_status[df_status['ENCOUNTERID'].isin(onset_filtered['ENCOUNTERID'])].merge(\n",
    "                             onset_filtered[['PATID','ENCOUNTERID','DISCHARGE_SINCE_ONSET', 'DEATH_SINCE_ONSET']], \n",
    "                             on = ['PATID', 'ENCOUNTERID'], \n",
    "                             how = 'outer')\n",
    "\n",
    "    mask_death_discharge = (\n",
    "                            (df_msm['DISCHARGE_SINCE_ONSET'] < df_msm['DAYS_SINCE_ONSET']) | \n",
    "                            (df_msm['DEATH_SINCE_ONSET'] < df_msm['DAYS_SINCE_ONSET'])\n",
    "                           ) \n",
    "    \n",
    "    df_msm = df_msm[~mask_death_discharge]\n",
    "\n",
    "    mask_discharge_same = (\n",
    "                        (df_msm['DISCHARGE_SINCE_ONSET'] == df_msm['DAYS_SINCE_ONSET']) | \n",
    "                        (df_msm['DAYS_SINCE_ONSET'].isna() & (df_msm['DISCHARGE_SINCE_ONSET'] <= 7))\n",
    "                      ) \n",
    "    df_msm.loc[mask_discharge_same, 'AKI_STATUS_CHANGE'] = 0\n",
    "    \n",
    "    mask_death_same = (\n",
    "                        (df_msm['DEATH_SINCE_ONSET'] == df_msm['DAYS_SINCE_ONSET']) | \n",
    "                        (df_msm['DAYS_SINCE_ONSET'].isna() & (df_msm['DEATH_SINCE_ONSET'] <= 7))\n",
    "                      ) \n",
    "    df_msm.loc[mask_death_same, 'AKI_STATUS_CHANGE'] = 4    \n",
    "\n",
    "    df_msm['AKI_STATUS_CHANGE'] = df_msm['AKI_STATUS_CHANGE'].fillna(2)\n",
    "    \n",
    "    discharge_list = onset[(onset['DISCHARGE_SINCE_ONSET'] <=7) & \n",
    "                           (onset['DISCHARGE_SINCE_ONSET'] < onset['DEATH_SINCE_ONSET'])].ENCOUNTERID.unique()\n",
    "    \n",
    "    mask_discharge  =     (\n",
    "                            (~df_msm.ENCOUNTERID.isin(df_msm[df_msm['AKI_STATUS_CHANGE'].isin([0, 4])].ENCOUNTERID.unique())) & \n",
    "                            (df_msm.ENCOUNTERID.isin(discharge_list)) \n",
    "                           )\n",
    "    \n",
    "    df_disc =  df_msm[mask_discharge].groupby('ENCOUNTERID').DISCHARGE_SINCE_ONSET.last().reset_index()\n",
    "    df_disc['AKI_STATUS_CHANGE'] = 0\n",
    "    df_disc['DAYS_SINCE_ONSET'] = df_disc['DISCHARGE_SINCE_ONSET'].copy()\n",
    "\n",
    "    death_list = onset[(onset['DEATH_SINCE_ONSET'] <=7) & \n",
    "                       (onset['DEATH_SINCE_ONSET'] <= onset['DISCHARGE_SINCE_ONSET'])].ENCOUNTERID.unique()\n",
    "    \n",
    "    mask_death   =      (\n",
    "                         (~df_msm.ENCOUNTERID.isin(df_msm[df_msm['AKI_STATUS_CHANGE'] == 4].ENCOUNTERID.unique())) & \n",
    "                         (df_msm.ENCOUNTERID.isin(death_list)) \n",
    "                        )\n",
    "    \n",
    "    df_death =  df_msm[mask_death].groupby('ENCOUNTERID').DEATH_SINCE_ONSET.last().reset_index()\n",
    "    df_death['AKI_STATUS_CHANGE'] = 4\n",
    "    df_death['DAYS_SINCE_ONSET'] = df_death['DEATH_SINCE_ONSET'].copy()\n",
    "    \n",
    "    df_final = pd.concat([df_msm[['ENCOUNTERID', 'DAYS_SINCE_ONSET', 'AKI_STATUS_CHANGE']], \n",
    "                          df_disc[['ENCOUNTERID', 'DAYS_SINCE_ONSET', 'AKI_STATUS_CHANGE']],\n",
    "                          df_death[['ENCOUNTERID', 'DAYS_SINCE_ONSET', 'AKI_STATUS_CHANGE']],\n",
    "                         ], axis=0)\n",
    "\n",
    "   \n",
    "    df_final['DAYS_SINCE_ONSET'] = df_final['DAYS_SINCE_ONSET'].astype('int')\n",
    "    df_final['AKI_STATUS_CHANGE'] = df_final['AKI_STATUS_CHANGE'].astype('int')\n",
    "\n",
    "    transitions = pd.DataFrame({\n",
    "        'TRANS':      [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "        'FROM_STATUS': [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3],\n",
    "        'TO_STATUS':   [0, 2, 3, 4, 0, 1, 3, 4, 0, 1, 2, 4]\n",
    "    })\n",
    "\n",
    "    df0 = df_final[['ENCOUNTERID', 'DAYS_SINCE_ONSET', 'AKI_STATUS_CHANGE']]\n",
    "    df0.sort_values(['ENCOUNTERID', 'DAYS_SINCE_ONSET'], inplace = True)\n",
    "\n",
    "    df0['tstart'] = df0.groupby('ENCOUNTERID')['DAYS_SINCE_ONSET'].shift(0)\n",
    "    df0['tstop'] = df0.groupby('ENCOUNTERID')['DAYS_SINCE_ONSET'].shift(-1)\n",
    "    df0['FROM_STATUS'] = df0['AKI_STATUS_CHANGE']\n",
    "    df0['TO_STATUS'] = df0.groupby('ENCOUNTERID')['AKI_STATUS_CHANGE'].shift(-1)\n",
    "\n",
    "    df1 = df0[~(df0.tstart == 7)]\n",
    "    df2 = df1[~(((df1.FROM_STATUS == 0) & (df1.tstop.isna())) | ((df1.FROM_STATUS == 4) & (df1.tstop.isna())))]\n",
    "\n",
    "    df2['tstop'] = df2['tstop'].fillna(1000)\n",
    "    df2['TO_STATUS'] = df2['TO_STATUS'].fillna(1000)\n",
    "\n",
    "    df2['transition_start'] = (\n",
    "        (df2['FROM_STATUS'] != df2['FROM_STATUS'].shift()) | \n",
    "        (df2['ENCOUNTERID'] != df2['ENCOUNTERID'].shift())\n",
    "    )\n",
    "\n",
    "    df2['transition_group'] = df2['transition_start'].cumsum()\n",
    "\n",
    "    df3 = df2.groupby(['ENCOUNTERID', 'transition_group']).agg(\n",
    "            tstart=('tstart', 'min'),                 \n",
    "            tstop=('tstop', 'max'),                   \n",
    "            FROM_STATUS=('FROM_STATUS', 'first'),     \n",
    "            TO_STATUS=('TO_STATUS', 'last')           \n",
    "        ).reset_index()\n",
    "\n",
    "    df3.sort_values(['ENCOUNTERID', 'tstart'], inplace=True)\n",
    "    df3.loc[(df3['FROM_STATUS'] == df3['TO_STATUS']), 'tstop'] = np.nan\n",
    "    df3.loc[(df3['FROM_STATUS'] == df3['TO_STATUS']), 'TO_STATUS'] = np.nan\n",
    "    df3['TO_STATUS'] = df3['TO_STATUS'].replace(1000, np.nan)\n",
    "    df3['tstop'] = df3['tstop'].replace(1000, np.nan)\n",
    "    df_trans = df3[['ENCOUNTERID', 'FROM_STATUS', 'TO_STATUS', 'tstart', 'tstop']]\n",
    "\n",
    "    df_censored = df_trans[df_trans.tstop.isna()].drop('TO_STATUS', axis = 1)\n",
    "    expanded_censored = df_censored[['ENCOUNTERID']].drop_duplicates().merge(transitions, how='cross')\n",
    "    df_censored_expanded = df_censored.merge(expanded_censored, on=['ENCOUNTERID', 'FROM_STATUS'], how='right')\n",
    "    df_censored_expanded = df_censored_expanded.dropna(subset=['tstart'])\n",
    "\n",
    "    df_censored_expanded['EVENT_STATUS'] = 0\n",
    "    df_censored_expanded['tstop'] = 7\n",
    "\n",
    "    df_unc = df_trans[~df_trans.tstop.isna()].drop_duplicates()\n",
    "    df_unc['tstop'] = df_unc['tstop'].astype(int)\n",
    "    df_unc['TO_STATUS']= df_unc['TO_STATUS'].astype(int)\n",
    "    df_unc['EVENT_STATUS'] = False\n",
    "    expanded_df = df_unc[['ENCOUNTERID']].drop_duplicates().merge(transitions, how='cross')\n",
    "\n",
    "    expanded_df2 = expanded_df.merge(df_unc, \n",
    "                                    on=['ENCOUNTERID', 'FROM_STATUS'], \n",
    "                                    how='left', \n",
    "                                    suffixes=('', '_orig'))\n",
    "    expanded_df2['EVENT_STATUS'] = expanded_df2['EVENT_STATUS'].fillna(False)\n",
    "    expanded_df2.loc[expanded_df2['TO_STATUS_orig'] == expanded_df2['TO_STATUS'], 'EVENT_STATUS'] = True\n",
    "    df_unc_expanded = expanded_df2.drop(columns=['TO_STATUS_orig']).dropna(subset = 'tstart').sort_values(['ENCOUNTERID', 'tstart', 'FROM_STATUS'])\n",
    "    \n",
    "    cols_to_keep = ['ENCOUNTERID', 'FROM_STATUS', 'TO_STATUS', 'TRANS', 'tstart', 'tstop', 'EVENT_STATUS']\n",
    "    data_surv =pd.concat([df_unc_expanded[cols_to_keep], df_censored_expanded[cols_to_keep]]).sort_values(by = ['ENCOUNTERID', 'tstart'])    \n",
    "    var_lst = ['ENCOUNTERID'] + imp_ft_lst \n",
    "\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    for t in range(0, 8):\n",
    "        if t < 7: \n",
    "            features = pd.read_pickle(file_paths[1] + 'data_mice_'+ 'd'+str(t+1) + '.pkl')\n",
    "        else: \n",
    "            features = pd.read_pickle(file_paths[1] + 'data_mice_'+ 'd'+str(7) + '.pkl')\n",
    "            \n",
    "        for var in imp_ft_lst:\n",
    "            if var not in features.columns:\n",
    "                features[var] = np.nan\n",
    "\n",
    "        features = features[var_lst]\n",
    "        df_day = data_surv[data_surv['tstart'] == t].merge(features, \n",
    "                                                          on = 'ENCOUNTERID', \n",
    "                                                          how = 'left')\n",
    "        df_all = pd.concat([df_all, df_day], axis = 0)\n",
    "\n",
    "    df_all['site'] = site_name\n",
    "    df_all['ENCOUNTERID'] = df_all['ENCOUNTERID'].astype(str)\n",
    "    \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3e615-74a8-4749-bffb-9abcc7da245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: Prepare data for multi-state transition analysis\n",
    "aki_subgrps = ['aki1', 'aki23']\n",
    "ft_aki1  = generate_ft_list(aki_subgrps[0])\n",
    "ft_aki23 = generate_ft_list(aki_subgrps[1])\n",
    "ft_all = sorted(list(set(ft_aki1) | set(ft_aki23)))\n",
    "\n",
    "for site_name in site_list:\n",
    "    impute_MICE(base_path, site_name, ft_all)\n",
    "\n",
    "for aki_subgrp in aki_subgrps:\n",
    "    data_all = pd.DataFrame()\n",
    "    ft_lst = generate_ft_list(aki_subgrp)\n",
    "    for site_name in site_list: \n",
    "        data_site = get_transition_data(base_path, site_name, ft_lst, aki_subgrp)\n",
    "        data_all = pd.concat([data_all, data_site], axis = 0)\n",
    "    data_all.to_parquet(base_path + 'data_msm_'+ aki_subgrp + '.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare data for the Sankey Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f15cebc86cd4fd86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_sankey_data(base_path, site_name, aki_subgrp):\n",
    "    filepath_lst = get_data_file_path(base_path, site_name)\n",
    "    onset0 = pd.read_pickle(filepath_lst[1]+'onset.pkl')\n",
    "    demo = pd.read_pickle(filepath_lst[0] + 'AKI_DEMO'+'.pkl')\n",
    "\n",
    "    demo_deduplicated = demo[['PATID', 'ENCOUNTERID', 'DEATH_DATE']].drop_duplicates()\n",
    "    demo_cleaned = (demo_deduplicated\n",
    "                    .groupby(['PATID'], as_index=False)\n",
    "                    .agg({'DEATH_DATE': lambda x: x.max() if x.notna().any() else pd.NaT}))\n",
    "    onset = onset0.merge(demo_cleaned[['PATID',  'DEATH_DATE']],\n",
    "                         on = 'PATID',\n",
    "                         how = 'left')\n",
    "\n",
    "    onset['DEATH_SINCE_ONSET'] = (onset['DEATH_DATE'] - onset['ONSET_DATE']).dt.days\n",
    "    onset['DEATH_SINCE_ONSET'] = onset['DEATH_SINCE_ONSET'].fillna(1000000)\n",
    "    onset['DEATH_SINCE_ONSET'] = onset['DEATH_SINCE_ONSET'].astype(int)\n",
    "    onset.loc[onset['DEATH_SINCE_ONSET'] < 0, 'DEATH_SINCE_ONSET'] = 1000000\n",
    "\n",
    "    if aki_subgrp == 'aki1':\n",
    "        onset = onset[onset['AKI_INIT_STG'] == 1]\n",
    "    elif aki_subgrp == 'aki23':\n",
    "        onset = onset[onset['AKI_INIT_STG'] >1]\n",
    "\n",
    "    df_scr = load_and_filter_scr(onset, filepath_lst)\n",
    "    df_scr0, df_admit = load_onset_data(filepath_lst)\n",
    "    df_rrt = get_rrt(df_admit, filepath_lst)\n",
    "    df_scr_filtered = df_scr[df_scr['DAYS_SINCE_ONSET'] >= 0]\n",
    "    df_scr_filtered = df_scr_filtered.merge(df_rrt[['PATID', 'ENCOUNTERID', 'RRT_ONSET_DATE']],\n",
    "                                            on =['PATID', 'ENCOUNTERID'],\n",
    "                                            how = 'left')\n",
    "    df_scr_filtered['RRT_SINCE_ONSET'] = (df_scr_filtered['RRT_ONSET_DATE'] - df_scr_filtered['ONSET_DATE']).dt.days\n",
    "\n",
    "    df_sankey = onset[['PATID','ENCOUNTERID','DISCHARGE_SINCE_ONSET', 'DEATH_SINCE_ONSET']].copy()\n",
    "\n",
    "    for t in range(0, 8):\n",
    "        df_t = df_scr_filtered[df_scr_filtered['DAYS_SINCE_ONSET'] <= t].sort_values(by = ['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ONSET']).groupby(['PATID', 'ENCOUNTERID']).last().reset_index()\n",
    "\n",
    "        # generate daily aki stage indicator (KDIGO definition)\n",
    "        status_label= 'State_d' + str(t)\n",
    "        df_t[status_label] = 1\n",
    "        mask_noaki = (df_t['RESULT_NUM'] < 1.5 * df_t['SCR_BASELINE']) & (df_t['RESULT_NUM'] < (0.3 + df_t['SCR_REFERENCE']))\n",
    "        mask_aki2 = (df_t['RESULT_NUM'] >= 2.0 * df_t['SCR_BASELINE']) & (df_t['RESULT_NUM'] < 3.0 * df_t['SCR_BASELINE'])\n",
    "        mask_aki3 = (\n",
    "                (df_t['RESULT_NUM']>=3.0 * df_t['SCR_BASELINE']) | (df_t['RESULT_NUM']>=4)  # scr criteria\n",
    "                | ((df_t['RRT_SINCE_ONSET'] <= df_t['DAYS_SINCE_ONSET']) & (df_t['RRT_SINCE_ONSET'] >= 0)) # rrt criteria\n",
    "        )\n",
    "        df_t.loc[mask_noaki,status_label] = 0\n",
    "        df_t.loc[mask_aki2,status_label] = 2\n",
    "        df_t.loc[mask_aki3,status_label] = 3\n",
    "\n",
    "        df_sankey = df_sankey.merge(df_t[['PATID','ENCOUNTERID', status_label]], on = ['PATID','ENCOUNTERID'], how = 'left')\n",
    "        df_sankey.loc[df_sankey['DISCHARGE_SINCE_ONSET'] <= t, status_label] = 4\n",
    "        df_sankey.loc[df_sankey['DEATH_SINCE_ONSET'] <= t, status_label] = 5\n",
    "        df_sankey[status_label] = df_sankey[status_label].fillna(1)\n",
    "\n",
    "    df_sankey['site'] = site_name\n",
    "    return df_sankey"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59e1c6bad0450e57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the Sankey data frame across all sites\n",
    "data_all = pd.DataFrame()\n",
    "for site_name in site_list:\n",
    "    data_site = get_sankey_data(base_path, site_name, 'all')\n",
    "    data_all = pd.concat([data_all, data_site], axis = 0)\n",
    "data_all.to_pickle(base_path + 'data_sankey' + '.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e166541b9f5e0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import (brier_score_loss, precision_recall_curve, roc_curve, roc_auc_score, auc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fb0eaa117682ce3"
  },
  {
   "cell_type": "markdown",
   "id": "a93ddfe4-395b-4077-a666-35d569043973",
   "metadata": {},
   "source": [
    "#### Part I: Collect Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2c887-aa36-4c21-adce-8abce116d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(pred, real, keep_all_cutoffs=False):\n",
    "    fpr_values, tpr_values, roc_thresholds = roc_curve(real, pred)\n",
    "    auroc = roc_auc_score(real, pred)\n",
    "\n",
    "    prec_values, sens_values, prc_thresholds = precision_recall_curve(real, pred)\n",
    "    auprc = auc(sens_values, prec_values)\n",
    "\n",
    "    perf_roc = pd.DataFrame({\n",
    "        'roc_cutoff': roc_thresholds,\n",
    "        'tpr': tpr_values,\n",
    "        'fpr': fpr_values\n",
    "    })\n",
    "\n",
    "    perf_prc = pd.DataFrame({\n",
    "        'prc_cutoff': np.append(prc_thresholds, 1.0),\n",
    "        'prec': prec_values,\n",
    "        'sens': sens_values\n",
    "    })\n",
    "\n",
    "    perf_summ = {\n",
    "        'auroc': auroc,\n",
    "        'auprc': auprc\n",
    "    }\n",
    "\n",
    "    perf_summ_df = pd.DataFrame(list(perf_summ.items()), columns=['overall_meas', 'meas_val'])\n",
    "\n",
    "    if keep_all_cutoffs:\n",
    "        return {'perf_summ':perf_summ_df, 'perf_roc': perf_roc, 'perf_prc': perf_prc}\n",
    "    else:\n",
    "        return {'perf_summ':perf_summ_df}\n",
    "\n",
    "def get_performance_curve(data, site_label, pred_pt, pred_task, fs_type, grp):\n",
    "    perf_at = get_performance_metrics(data['pred'], data['y'], keep_all_cutoffs=True)\n",
    "    perf_roc_data = perf_at['perf_roc']\n",
    "    perf_roc_data['site'] = site_label\n",
    "    perf_roc_data['pred_pt'] = pred_pt\n",
    "    perf_roc_data['pred_task'] = pred_task\n",
    "    perf_roc_data['fs_type'] = fs_type\n",
    "    perf_roc_data['grp'] = grp\n",
    "    perf_roc_data['meas_value'] = perf_at['perf_summ'][perf_at['perf_summ']['overall_meas'] == 'auroc']['meas_val'].values[0]\n",
    "\n",
    "    # Calculate performance at all cutoffs\n",
    "    perf_prc_data = perf_at['perf_prc']\n",
    "    perf_prc_data['site'] = site_label\n",
    "    perf_prc_data['pred_pt'] = pred_pt\n",
    "    perf_prc_data['pred_task'] = pred_task\n",
    "    perf_prc_data['fs_type'] = fs_type\n",
    "    perf_prc_data['grp'] = grp\n",
    "    perf_prc_data['auroc'] = perf_at['perf_summ'][perf_at['perf_summ']['overall_meas'] == 'auprc']['meas_val'].values[0]\n",
    "    return perf_roc_data, perf_prc_data\n",
    "\n",
    "def get_calibration_curve(pred, real, n_bin=20, cal_set = True, iso_reg_model = None):\n",
    "    calib = pd.DataFrame({'pred': pred, 'y': real})\n",
    "\n",
    "    calib = calib.sort_values(by='pred').reset_index(drop=True)\n",
    "\n",
    "    calib['pred_bin'] = pd.cut(calib['pred'],\n",
    "                               bins=np.unique(np.quantile(calib['pred'], np.linspace(0, 1, n_bin + 1))),\n",
    "                               include_lowest=True,\n",
    "                               labels=False) + 1  # Add 1 to labels to start binning from 1\n",
    "\n",
    "    calib_summary = calib.groupby('pred_bin').apply(lambda df: pd.Series({\n",
    "        'expos': len(df),\n",
    "        'bin_lower': df['pred'].min(),\n",
    "        'bin_upper': df['pred'].max(),\n",
    "        'bin_mid': df['pred'].median(),\n",
    "        'y_agg': df['y'].sum(),\n",
    "        'pred_p': df['pred'].mean()\n",
    "    })).reset_index()\n",
    "\n",
    "    brier_stat_uncal = brier_score_loss(calib['y'], calib['pred'])\n",
    "    calib_summary['brier_uncal'] = brier_stat_uncal\n",
    "    calib_summary['y_p'] = calib_summary['y_agg'] / calib_summary['expos']\n",
    "    calib_summary['binCI_lower'] = np.maximum(0, calib_summary['pred_p'] - 1.96 * np.sqrt(calib_summary['y_p'] * (1 - calib_summary['y_p']) / calib_summary['expos']))\n",
    "    calib_summary['binCI_upper'] = calib_summary['pred_p'] + 1.96 * np.sqrt(calib_summary['y_p'] * (1 - calib_summary['y_p']) / calib_summary['expos'])\n",
    "\n",
    "    if cal_set:\n",
    "        # Fit isotonic regression model\n",
    "        iso_reg_cal = IsotonicRegression(out_of_bounds='clip')\n",
    "        iso_reg_cal.fit(calib_summary['bin_mid'], calib_summary['y_p'])\n",
    "        return iso_reg_cal\n",
    "    else: \n",
    "        # Apply recalibration to original predicted probabilities\n",
    "        calib['pred_recal'] = iso_reg_model.predict(calib['pred']) \n",
    "        brier_stat_recal = brier_score_loss(calib['y'], calib['pred_recal'])\n",
    "        calib_summary['brier_recal'] = brier_stat_recal\n",
    "        calib_summary['pred_recal'] = iso_reg_model.predict(calib_summary['bin_mid'])\n",
    "        return calib_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409fafc-3ec8-49c7-846d-e3737d35da61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_performance_metrics(model_label, pred_pt, aki_subgrp, site_labels, subset = True, n_boots = 199):\n",
    "    # Get bootstrapped AUROC and AUPRC\n",
    "    roc_curve_tbl = pd.DataFrame()\n",
    "    prc_curve_tbl = pd.DataFrame()\n",
    "    perf_tbl = pd.DataFrame()\n",
    "    \n",
    "    pred_task_lst = ['rvsl', 'stgup']\n",
    "    fs_type_opt = ['no_fs', 'rm_scr_bun']  \n",
    "    perf_metrics = ['auroc', 'auprc']\n",
    "\n",
    "    for site_label in site_labels:\n",
    "        for pred_task in pred_task_lst:\n",
    "            if pred_task == 'rvsl':\n",
    "                outcome_label = 'AKI_RVRT'\n",
    "            elif pred_task == 'stgup':\n",
    "                outcome_label = 'AKI_STGUP'\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pred_task: {pred_task}\")\n",
    "            \n",
    "            for fs_type in fs_type_opt:\n",
    "                result_dict = pd.read_pickle(model_path + model_label + '_' + aki_subgrp +'_' + pred_task + '_' + str(pred_pt + 1) + 'd_' + fs_type + '.pkl')\n",
    "                if model_label in ['rlr', 'rlr_tmpr']:\n",
    "                    data_label = 'data_test_raw'\n",
    "                else: \n",
    "                    data_label = 'data_test'\n",
    "                valid_orig = result_dict[site_label][data_label]\n",
    "                valid_orig['pred'] = result_dict[site_label]['y_pred']\n",
    "                valid_orig['y'] = valid_orig[outcome_label]\n",
    "                valid_orig['age'] = pd.cut(valid_orig['AGE'], bins=[0, 45, 65, np.inf], right=False)\n",
    "\n",
    "                valid_orig['sex'] = np.where(valid_orig['MALE'], \n",
    "                                                        'Male',\n",
    "                                                        'Female')\n",
    "                valid_orig['race'] = np.where(valid_orig['RACE_WHITE'], \n",
    "                                        'White',\n",
    "                                        'Nonwhite')\n",
    "                valid_orig['ckd_stg'] = np.where(valid_orig['PREADM_CKD_FLAG'], \n",
    "                                                        'CKD',\n",
    "                                                        'Non_CKD')\n",
    "                valid_orig['hispanic'] = np.where(valid_orig['HISPANIC'], \n",
    "                                        'HISPANIC',\n",
    "                                        'Non_HISPANIC')\n",
    "                valid_orig['pod'] = np.where(valid_orig['ONSET_SINCE_ADMIT'] == 0, \n",
    "                                        'ADM_DAY_ONSET',\n",
    "                                        'POST_ADM_DAY_ONSET')                \n",
    "\n",
    "                overall_roc, overall_prc = get_performance_curve(valid_orig, site_label, pred_pt, pred_task, fs_type, 'Overall')\n",
    "                \n",
    "                roc_curve_tbl = pd.concat([roc_curve_tbl, overall_roc], ignore_index=True)  \n",
    "                prc_curve_tbl = pd.concat([prc_curve_tbl, overall_prc], ignore_index=True)\n",
    "                \n",
    "                ## Subgroup analysis\n",
    "                if subset:\n",
    "                    # Subgroup by age\n",
    "                    subgrp_roc_age = pd.DataFrame()\n",
    "                    subgrp_prc_age = pd.DataFrame()\n",
    "                    for grp in valid_orig['age'].unique():\n",
    "                        valid_grp = valid_orig[valid_orig['age'] == grp]\n",
    "                        grp_roc, grp_prc = get_performance_curve(valid_grp, site_label, pred_pt, pred_task, fs_type, f\"Subgrp_age:{grp}\")\n",
    "                        subgrp_roc_age = pd.concat([subgrp_roc_age, grp_roc], ignore_index=True)\n",
    "                        subgrp_prc_age = pd.concat([subgrp_prc_age, grp_prc], ignore_index=True)\n",
    "\n",
    "                    # Subgroup by sex\n",
    "                    subgrp_roc_sex = pd.DataFrame()\n",
    "                    subgrp_prc_sex = pd.DataFrame()\n",
    "                    for grp in valid_orig['sex'].unique():\n",
    "                        valid_grp = valid_orig[valid_orig['sex'] == grp]\n",
    "                        grp_roc, grp_prc = get_performance_curve(valid_grp, site_label, pred_pt, pred_task, fs_type, f\"Subgrp_sex:{grp}\")\n",
    "                        subgrp_roc_sex = pd.concat([subgrp_roc_sex, grp_roc], ignore_index=True)\n",
    "                        subgrp_prc_sex = pd.concat([subgrp_prc_sex, grp_prc], ignore_index=True)\n",
    "\n",
    "                    # Subgroup by day of event\n",
    "                    subgrp_roc_pod = pd.DataFrame()\n",
    "                    subgrp_prc_pod = pd.DataFrame()\n",
    "                    for grp in valid_orig['pod'].unique():\n",
    "                        valid_grp = valid_orig[valid_orig['pod'] == grp]\n",
    "                        grp_roc, grp_prc = get_performance_curve(valid_grp, site_label, pred_pt, pred_task, fs_type, f\"Subgrp_pod:{grp}\")\n",
    "                        subgrp_roc_pod = pd.concat([subgrp_roc_pod, grp_roc], ignore_index=True)\n",
    "                        subgrp_prc_pod = pd.concat([subgrp_prc_pod, grp_prc], ignore_index=True)\n",
    "\n",
    "                    # Combine all results\n",
    "                    roc_curve_tbl = pd.concat([roc_curve_tbl, subgrp_roc_age, subgrp_roc_sex, subgrp_roc_pod], ignore_index=True)\n",
    "                    prc_curve_tbl = pd.concat([prc_curve_tbl, subgrp_prc_age, subgrp_prc_sex, subgrp_prc_pod], ignore_index=True)\n",
    "                ##\n",
    "\n",
    "                # Bootstrap the performance metrics\n",
    "                for b in range(n_boots):\n",
    "                    rng_boot = b + 1234\n",
    "                    valid = valid_orig.sample(frac=1, replace=True, random_state = rng_boot)\n",
    "                    # Overall performance metrics\n",
    "                    perf_overall = get_performance_metrics(valid['pred'], valid['y'], keep_all_cutoffs=False)['perf_summ']\n",
    "                    perf_overall = perf_overall[perf_overall['overall_meas'].isin(perf_metrics)]\n",
    "                    perf_overall['size'] = len(valid)\n",
    "                    perf_overall['grp'] = \"Overall\"\n",
    "                    perf_overall['pred_task'] = pred_task\n",
    "\n",
    "                    ## Subgroup analysis\n",
    "                    if subset:\n",
    "                        # Subgroup by age group\n",
    "                        subgrp_age = pd.DataFrame()\n",
    "\n",
    "                        valid = valid_orig.groupby('age', group_keys=False).apply(\n",
    "                            lambda x: x.sample(frac=1, replace=True, random_state = rng_boot))\n",
    "\n",
    "                        for grp in valid['age'].unique():\n",
    "                            valid_grp = valid[valid['age'] == grp]\n",
    "                            grp_summ = get_performance_metrics(valid_grp['pred'], valid_grp['y'], keep_all_cutoffs=False)['perf_summ']\n",
    "                            grp_summ = grp_summ[grp_summ['overall_meas'].isin(perf_metrics)]\n",
    "                            grp_summ['size'] = len(valid_grp)\n",
    "                            grp_summ['grp'] = f\"Subgrp_age:{grp}\"\n",
    "                            grp_summ['pred_task'] = pred_task\n",
    "                            subgrp_age = pd.concat([subgrp_age, grp_summ], ignore_index=True)\n",
    "                            \n",
    "                        # Subgroup by sex\n",
    "                        subgrp_sex = pd.DataFrame()\n",
    "\n",
    "                        valid = valid_orig.groupby('sex', group_keys=False).apply(\n",
    "                            lambda x: x.sample(frac=1, replace=True, random_state = rng_boot))\n",
    "\n",
    "                        for grp in valid['sex'].unique():\n",
    "                            valid_grp = valid[valid['sex'] == grp]\n",
    "                            grp_summ = get_performance_metrics(valid_grp['pred'], valid_grp['y'], keep_all_cutoffs=False)['perf_summ']\n",
    "                            grp_summ = grp_summ[grp_summ['overall_meas'].isin(perf_metrics)]\n",
    "                            grp_summ['size'] = len(valid_grp)\n",
    "                            grp_summ['grp'] = f\"Subgrp_sex:{grp}\"\n",
    "                            grp_summ['pred_task'] = pred_task\n",
    "                            subgrp_sex = pd.concat([subgrp_sex, grp_summ], ignore_index=True)\n",
    "\n",
    "                        # Subgroup by race\n",
    "                        subgrp_race = pd.DataFrame()\n",
    "\n",
    "                        valid = valid_orig.groupby('race', group_keys=False).apply(\n",
    "                            lambda x: x.sample(frac=1, replace=True, random_state = rng_boot))\n",
    "\n",
    "                        for grp in valid['race'].unique():\n",
    "                            valid_grp = valid[valid['race'] == grp]\n",
    "                            grp_summ = get_performance_metrics(valid_grp['pred'], valid_grp['y'], keep_all_cutoffs=False)['perf_summ']\n",
    "                            grp_summ = grp_summ[grp_summ['overall_meas'].isin(perf_metrics)]\n",
    "                            grp_summ['size'] = len(valid_grp)\n",
    "                            grp_summ['grp'] = f\"Subgrp_race:{grp}\"\n",
    "                            grp_summ['pred_task'] = pred_task\n",
    "                            subgrp_race = pd.concat([subgrp_race, grp_summ], ignore_index=True)\n",
    "    \n",
    "                        # Subgroup by day of event\n",
    "                        subgrp_pod = pd.DataFrame()\n",
    "\n",
    "                        valid = valid_orig.groupby('pod', group_keys=False).apply(\n",
    "                            lambda x: x.sample(frac=1, replace=True, random_state = rng_boot))\n",
    "\n",
    "                        for grp in valid['pod'].unique():\n",
    "                            valid_grp = valid[valid['pod'] == grp]\n",
    "                            grp_summ = get_performance_metrics(valid_grp['pred'], valid_grp['y'], keep_all_cutoffs=False)['perf_summ']\n",
    "                            grp_summ = grp_summ[grp_summ['overall_meas'].isin(perf_metrics)]\n",
    "                            grp_summ['size'] = len(valid_grp)\n",
    "                            grp_summ['grp'] = f\"Subgrp_pod:{grp}\"\n",
    "                            grp_summ['pred_task'] = pred_task\n",
    "                            subgrp_pod = pd.concat([subgrp_pod, grp_summ], ignore_index=True)\n",
    "\n",
    "                        # Combine all results\n",
    "                        perf_overall = pd.concat([perf_overall, subgrp_age, subgrp_sex, subgrp_race,  subgrp_pod], ignore_index=True)\n",
    "                        ##\n",
    "                    perf_overall['pred_pt']  =  pred_pt\n",
    "                    perf_overall['fs_type']  =  fs_type\n",
    "                    perf_overall['site']     =  site_label\n",
    "                    perf_overall['meas_val'] =  perf_overall['meas_val'].round(4)\n",
    "\n",
    "                    perf_tbl = pd.concat([perf_tbl, perf_overall.assign(boots=b)], ignore_index=True)\n",
    "                    \n",
    "    return {'roc_curve' : roc_curve_tbl,\n",
    "            'prc_curve' : prc_curve_tbl,\n",
    "            'perf_tbl': perf_tbl}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837a117-8116-48e8-bce7-d46e92093225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_calibration_results(model_label, pred_pt, aki_subgrp, site_labels, n_boots):\n",
    "    calib_all = pd.DataFrame()\n",
    "    \n",
    "    pred_task_lst = ['rvsl', 'stgup']\n",
    "    fs_type_opt = ['no_fs', 'rm_scr_bun'] \n",
    "    \n",
    "    for site_label in site_labels:\n",
    "        for pred_task in pred_task_lst:\n",
    "            if pred_task == 'rvsl':\n",
    "                outcome_label = 'AKI_RVRT'\n",
    "            elif pred_task == 'stgup':\n",
    "                outcome_label = 'AKI_STGUP'\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pred_task: {pred_task}\")\n",
    "\n",
    "            for fs_type in fs_type_opt:\n",
    "                result_dict = pd.read_pickle(model_path + model_label + '_' + aki_subgrp +'_' + pred_task + '_' + str(pred_pt + 1) + 'd_' + fs_type + '.pkl')\n",
    "\n",
    "                valid_orig = result_dict[site_label]['data_test']\n",
    "                valid_orig['y_test_pred'] = result_dict[site_label]['y_pred']\n",
    "                valid_orig['y_test'] = valid_orig[outcome_label]\n",
    "                df_cal = result_dict[site_label]['data_val']\n",
    "            \n",
    "                if model_label == 'cb':\n",
    "                    x_cal = df_cal.drop(['ID_POD', 'ID_PAT_ENC', 'PATID', 'ENCOUNTERID', outcome_label], axis = 1)\n",
    "                elif  model_label == 'cb_tmpr':\n",
    "                    x_cal = df_cal.drop(['ID_POD', 'ID_PAT_ENC', 'PATID', 'ENCOUNTERID', 'BCCOVID', outcome_label], axis = 1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model: {model_label}\")\n",
    "\n",
    "                y_cal_pred = result_dict[site_label]['best_model'].predict_proba(x_cal)[:, 1]\n",
    "                y_cal = result_dict[site_label]['data_val'][outcome_label]\n",
    "\n",
    "                recal_model = get_calibration_curve(y_cal_pred, y_cal, cal_set = True)\n",
    "                calib_orig = get_calibration_curve(valid_orig['y_test_pred'], valid_orig['y_test'], \n",
    "                           cal_set = False, \n",
    "                           iso_reg_model =recal_model)\n",
    "                calib_tbl = pd.DataFrame()\n",
    "\n",
    "                # Bootstrap the calibration metrics\n",
    "                for b in range(n_boots):\n",
    "                    rng_boot = b + 1234\n",
    "                    valid = valid_orig.sample(frac=1, replace=True, random_state = rng_boot)\n",
    "\n",
    "                    calib = get_calibration_curve(valid['y_test_pred'], valid['y_test'], cal_set = False, iso_reg_model =recal_model)\n",
    "                    calib['boots'] = b\n",
    "                    calib_tbl = pd.concat([calib_tbl, calib], ignore_index=True)\n",
    "            \n",
    "                calib_tbl['brier_orig'] = calib_orig['brier_uncal'].values[0]\n",
    "                calib_tbl['brier_recal_orig'] = calib_orig['brier_recal'].values[0]\n",
    "                calib_tbl['site'] = site_label\n",
    "                calib_tbl['fs_type'] = fs_type\n",
    "                calib_tbl['pred_task'] = pred_task\n",
    "                calib_all =  pd.concat([calib_all, calib_tbl], ignore_index=True)\n",
    "\n",
    "    return calib_all             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set paths\n",
    "base_path = './'\n",
    "model_path = os.path.join(base_path, 'model') + '/'\n",
    "result_path = os.path.join(base_path, 'result') + '/'\n",
    "\n",
    "# Collect performance metrics\n",
    "nboots = 99\n",
    "site_labels = ['Site1', 'Site2', 'Site3', 'Site4']\n",
    "\n",
    "perf_aki1_dict = {'cb': collect_performance_metrics('cb', 0, 'aki1', site_labels, subset=True, n_boots=nboots),\n",
    "                  'cb_tmpr': collect_performance_metrics('cb_tmpr', 0, 'aki1', site_labels, subset=True,\n",
    "                                                         n_boots=nboots),\n",
    "                  'rlr': collect_performance_metrics('rlr', 0, 'aki1', site_labels, subset=True, n_boots=nboots),\n",
    "                  'rlr_tmpr': collect_performance_metrics('rlr_tmpr', 0, 'aki1', site_labels, subset=True,\n",
    "                                                          n_boots=nboots)}\n",
    "perf_aki23_dict = {'cb': collect_performance_metrics('cb', 0, 'aki23', site_labels, subset=True, n_boots=nboots),\n",
    "                   'cb_tmpr': collect_performance_metrics('cb_tmpr', 0, 'aki23', site_labels, subset=True,\n",
    "                                                          n_boots=nboots),\n",
    "                   'rlr': collect_performance_metrics('rlr', 0, 'aki23', site_labels, subset=True, n_boots=nboots),\n",
    "                   'rlr_tmpr': collect_performance_metrics('rlr_tmpr', 0, 'aki23', site_labels, subset=True,\n",
    "                                                           n_boots=nboots)}\n",
    "\n",
    "# Collect calibration results\n",
    "calibtbl = {'aki1': {}, 'aki23': {}}\n",
    "\n",
    "calibtbl['aki1']['rnd'] = collect_calibration_results('cb', 0, 'aki1', site_labels, nboots)\n",
    "calibtbl['aki23']['rnd'] = collect_calibration_results('cb', 0, 'aki23', site_labels, nboots)\n",
    "calibtbl['aki1']['tmpr'] = collect_calibration_results('cb_tmpr', 0, 'aki1', site_labels, nboots)\n",
    "calibtbl['aki23']['tmpr'] = collect_calibration_results('cb_tmpr', 0, 'aki23', site_labels, nboots)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2bae5fdfafbf456"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Part II: Create Plot of ROC/PRC Curves"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e1c4c0e94cf9937"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8add991-25f5-4443-9b08-f869c5995325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_pm_cross_site(perf_dicts, aki_subgrp, meas_type, result_path):\n",
    "    scenarios = ['cb', 'cb_tmpr', 'rlr', 'rlr_tmpr']\n",
    "    scenario_labels = {'cb':'Catboost, Int. Val.', \n",
    "                       'cb_tmpr':'Catboost, Temp. Val.',\n",
    "                       'rlr': 'Logistic Reg., Int. Val.',\n",
    "                       'rlr_tmpr': 'Logistic Reg., Temp. Val.'}  \n",
    "\n",
    "    subplot_config = [\n",
    "            ('rvsl', 'Reversal',    'no_fs',      ' (all fts.)'),\n",
    "            ('rvsl', 'Reversal',    'rm_scr_bun', ' (no SCr/BUN)'),\n",
    "            ('stgup', 'Progression', 'no_fs',      ' (all fts.)'),\n",
    "            ('stgup', 'Progression', 'rm_scr_bun', ' (no SCr/BUN)')\n",
    "        ]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=4, ncols= len(scenarios), figsize=(16, 12), sharex=True, sharey=True)\n",
    "\n",
    "    for col, scenario in enumerate(scenarios):\n",
    "        perf_dict = perf_dicts[scenario]\n",
    "        perf_tbl = perf_dict['perf_tbl'].groupby(\n",
    "            ['site', 'pred_pt', 'pred_task', 'fs_type', 'grp', 'overall_meas']\n",
    "        ).agg({\n",
    "            'size': 'mean',\n",
    "            'meas_val': ['median', lambda x: np.quantile(x, 0.025), lambda x: np.quantile(x, 0.975)]\n",
    "        }).reset_index()\n",
    "        perf_tbl.columns = ['site','pred_pt', 'pred_task', 'fs_type', 'grp', 'overall_meas', 'size', 'meas_med', 'meas_lb', 'meas_ub']\n",
    "\n",
    "        for row, (pred_task, task_label, fs, fs_label) in enumerate(subplot_config):\n",
    "            ax = axes[row, col]\n",
    "            curve_data = perf_dict[meas_type + '_curve']\n",
    "            curve_data = curve_data[\n",
    "                (curve_data['pred_task'] == pred_task) &\n",
    "                (curve_data['grp'] == 'Overall') &\n",
    "                (curve_data['fs_type'] == fs)\n",
    "            ]\n",
    "            tbl_data = perf_tbl[\n",
    "                (perf_tbl['overall_meas'] == ('au' + meas_type)) &\n",
    "                (perf_tbl['pred_task'] == pred_task) &\n",
    "                (perf_tbl['grp'] == 'Overall') &\n",
    "                (perf_tbl['fs_type'] == fs)\n",
    "            ]\n",
    "\n",
    "            for site, site_data in curve_data.groupby('site'):\n",
    "                boot_meas = tbl_data[tbl_data['site'] == site]\n",
    "                site_label = site + ':' + f\"{np.round(boot_meas['meas_med'].values[0],2)} ({np.round(boot_meas['meas_lb'].values[0],2)}, {np.round(boot_meas['meas_ub'].values[0],2)})\"\n",
    "                x_data = site_data['sens'] if meas_type == 'prc' else site_data['fpr']\n",
    "                y_data = site_data['prec'] if meas_type == 'prc' else site_data['tpr']\n",
    "\n",
    "                line, = ax.plot(x_data, y_data, label=site_label)\n",
    "                ax.plot(x_data[::120], y_data[::120], '^', color=line.get_color())  # Markers\n",
    "\n",
    "            if meas_type == 'roc':\n",
    "                ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "            ax.set_title(f' {task_label + fs_label} : {scenario_labels[scenario]}', fontsize=9)\n",
    "            ax.legend(loc='lower right' if meas_type == 'roc' else 'upper right', fontsize='small')\n",
    "\n",
    "    x_label = 'False Positive Rate' if meas_type == 'roc' else 'Recall'\n",
    "    y_label = 'True Positive Rate'  if meas_type == 'roc' else 'Precision'\n",
    "    \n",
    "    fig.text(0.5, 0.04, x_label, ha='center', va='center', fontsize=12)\n",
    "    fig.text(0.04, 0.5, y_label, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "    plt.tight_layout(rect=(0.05, 0.05, 1, 1))\n",
    "\n",
    "    figure_filename = os.path.join(result_path, 'figure', 'plot_eval_'+ aki_subgrp + meas_type + '.png')\n",
    "    plt.savefig(figure_filename, bbox_inches='tight', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542bd0b-5079-47c2-bb84-535cfc233f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC and PRC curves \n",
    "# AKI 1 at onset\n",
    "plot_pm_cross_site(perf_aki1_dict, 'aki1', 'roc', result_path)\n",
    "plot_pm_cross_site(perf_aki1_dict, 'aki1', 'prc', result_path)\n",
    "# AKI 2&3 at onset\n",
    "plot_pm_cross_site(perf_aki23_dict, 'aki23', 'roc', result_path)\n",
    "plot_pm_cross_site(perf_aki23_dict, 'aki23', 'prc', result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9174c4-64d2-4ebc-8876-06c7c8372a2b",
   "metadata": {},
   "source": [
    "#### Part III: Create Plot of Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b959a-b2a1-4116-b042-e52b3c030117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration_curve(calib_rnd_tbl, calib_tmpr_tbl, aki_subgrp, site_labels, result_path=\".\"):\n",
    "    task_labels = {'rvsl': 'Reversal', 'stgup': 'Progression'}\n",
    "    pred_task_lst = list(task_labels.keys()) \n",
    "\n",
    "    fs_labels = {'no_fs': 'all fts.', 'rm_scr_bun': 'no SCr/BUN'}\n",
    "    fs_type_opt = list(fs_labels.keys())    \n",
    "\n",
    "    subplot_titles = [(pred_task, fs_type) \n",
    "                      for pred_task in pred_task_lst \n",
    "                      for fs_type in fs_type_opt]\n",
    "    \n",
    "    n_rows, n_cols = 2, 4\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "    global_legend = [\n",
    "        plt.Line2D([0], [0], linestyle='--', color='k', label='Uncalibrated'),\n",
    "        plt.Line2D([0], [0], linestyle='-', color='k', label='Recalibrated')\n",
    "    ]\n",
    "    \n",
    "    def plot_one_subplt(ax, calib_tbl, pred_task, fs_type, site_labels, \n",
    "                        task_labels, fs_labels, row_label):\n",
    "\n",
    "        calib_data = calib_tbl[\n",
    "            (calib_tbl['pred_task'] == pred_task) &\n",
    "            (calib_tbl['fs_type'] == fs_type)\n",
    "        ]\n",
    "        \n",
    "        agg_data = calib_data.groupby(['pred_bin', 'site']).agg(\n",
    "            median_pred_p=('pred_recal', 'median'),  # Recalibrated probability\n",
    "            median_pred_uncal=('bin_mid', 'median'), # Uncalibrated bin midpoint\n",
    "            median_y_p=('y_p', 'median'),            # Observed probability\n",
    "            y_p_lower=('y_p', lambda x: np.quantile(x, 0.025)),\n",
    "            y_p_upper=('y_p', lambda x: np.quantile(x, 0.975))\n",
    "        ).reset_index()\n",
    "        \n",
    "        for site in site_labels:\n",
    "            site_data = agg_data[agg_data['site'] == site]\n",
    "            if site_data.empty:\n",
    "                continue\n",
    "            \n",
    "            site_rows = calib_data[calib_data['site'] == site]\n",
    "            if 'brier_orig' in site_rows.columns:\n",
    "                brier_orig = site_rows['brier_orig'].iloc[0]\n",
    "            else:\n",
    "                brier_orig = np.nan\n",
    "            \n",
    "            if 'brier_uncal' in site_rows.columns:\n",
    "                brier_cil = site_rows['brier_uncal'].quantile(0.025)\n",
    "                brier_ciu = site_rows['brier_uncal'].quantile(0.975)\n",
    "            else:\n",
    "                brier_cil, brier_ciu = np.nan, np.nan\n",
    "\n",
    "            color_idx = site_labels.index(site)\n",
    "\n",
    "            site_label = f\"{site}:{brier_orig:.2f} ({brier_cil:.2f}, {brier_ciu:.2f})\"\n",
    "            \n",
    "            ax.errorbar(\n",
    "                site_data['median_pred_uncal'], \n",
    "                site_data['median_y_p'],\n",
    "                yerr=[\n",
    "                    site_data['median_y_p'] - site_data['y_p_lower'],\n",
    "                    site_data['y_p_upper'] - site_data['median_y_p']\n",
    "                ],\n",
    "                fmt='--o', color=f\"C{color_idx}\", alpha=0.35, \n",
    "                capsize=2, label=None\n",
    "            )\n",
    "            \n",
    "            ax.errorbar(\n",
    "                site_data['median_pred_p'], \n",
    "                site_data['median_y_p'],\n",
    "                yerr=[\n",
    "                    site_data['median_y_p'] - site_data['y_p_lower'],\n",
    "                    site_data['y_p_upper'] - site_data['median_y_p']\n",
    "                ],\n",
    "                fmt='-o', color=f\"C{color_idx}\", capsize=2, label=site_label\n",
    "            )\n",
    "        \n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Perfect Calibration')\n",
    "        ax.set_title(f\"{task_labels[pred_task]} ({fs_labels[fs_type]}): {row_label}\", fontsize=10)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(loc='upper left', fontsize=7.5)\n",
    "    \n",
    "    # Row 0: Random validation\n",
    "    for col_idx, (pred_task, fs_type) in enumerate(subplot_titles):\n",
    "        ax = axes[0, col_idx]\n",
    "        plot_one_subplt(\n",
    "            ax=ax,\n",
    "            calib_tbl=calib_rnd_tbl,\n",
    "            pred_task=pred_task,\n",
    "            fs_type=fs_type,\n",
    "            site_labels=site_labels,\n",
    "            task_labels=task_labels,\n",
    "            fs_labels=fs_labels,\n",
    "            row_label=\"Int. Val.\"\n",
    "        )\n",
    "\n",
    "    # Row 1: Temporal validation\n",
    "    for col_idx, (pred_task, fs_type) in enumerate(subplot_titles):\n",
    "        ax = axes[1, col_idx]\n",
    "        plot_one_subplt(\n",
    "            ax=ax,\n",
    "            calib_tbl=calib_tmpr_tbl,\n",
    "            pred_task=pred_task,\n",
    "            fs_type=fs_type,\n",
    "            site_labels=site_labels,\n",
    "            task_labels=task_labels,\n",
    "            fs_labels=fs_labels,\n",
    "            row_label=\"Temp. Val.\"\n",
    "        )\n",
    "    \n",
    "    x_label = \"Predicted Probability\"\n",
    "    y_label = \"Observed Probability\"\n",
    "    fig.text(0.5, 0.06, x_label, ha='center', va='center', fontsize=12)\n",
    "    fig.text(0.01, 0.5, y_label, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "    \n",
    "    fig.legend(\n",
    "        handles=global_legend, \n",
    "        loc='lower center', \n",
    "        ncol=2, \n",
    "        fontsize=10, \n",
    "        bbox_to_anchor=(0.5, 0)\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0.04, top=0.9, bottom=0.12)\n",
    "    \n",
    "    figure_filename = os.path.join(result_path, 'figure', f'plot_calbr_{aki_subgrp}.png')\n",
    "    plt.savefig(figure_filename, bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7e13f-3ba9-47d2-94bd-a2bea103a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot of calibration curves\n",
    "# AKI 1\n",
    "plot_calibration_curve(\n",
    "    calib_rnd_tbl=calibtbl['aki1']['rnd'],\n",
    "    calib_tmpr_tbl=calibtbl['aki1']['tmpr'],\n",
    "    aki_subgrp='aki1',\n",
    "    site_labels=site_labels,\n",
    "    result_path=result_path\n",
    ")\n",
    "# AKI 2&3\n",
    "plot_calibration_curve(\n",
    "    calib_rnd_tbl=calibtbl['aki23']['rnd'],\n",
    "    calib_tmpr_tbl=calibtbl['aki23']['tmpr'],\n",
    "    aki_subgrp='aki23',\n",
    "    site_labels=site_labels,\n",
    "    result_path=result_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b9549-b712-4bea-8749-e3d746261554",
   "metadata": {},
   "source": [
    "#### Part IV: Subgroup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafcfa3-67da-40d9-b56c-3af30c3c5614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_subgrp_table(perf_dict, aki_subgrp, pred_task, meas_type):\n",
    "    perf_tbl = perf_dict['perf_tbl'].groupby(['site', 'pred_pt', 'pred_task', 'fs_type', 'grp', 'overall_meas']).agg({\n",
    "        'size': 'mean',\n",
    "        'meas_val': ['median', lambda x: np.quantile(x, 0.025), lambda x: np.quantile(x, 0.975)]\n",
    "    }).reset_index()\n",
    "    perf_tbl.columns = ['site','pred_pt', 'pred_task', 'fs_type', 'grp', 'overall_meas', 'size', 'meas_med', 'meas_lb', 'meas_ub']\n",
    "\n",
    "\n",
    "    perf_tbl_filtered = perf_tbl[(perf_tbl['fs_type'] == 'no_fs') & (perf_tbl['overall_meas'] == meas_type) & (perf_tbl['pred_task'] == pred_task)]\n",
    "    def format_meas(row):\n",
    "        median = np.round(row['meas_med'], 2)\n",
    "        lb = np.round(row['meas_lb'], 2) \n",
    "        ub = np.round(row['meas_ub'], 2) \n",
    "        return f\"{median} ({lb}, {ub})\"\n",
    "\n",
    "    perf_tbl_filtered['meas_formatted'] = perf_tbl_filtered.apply(format_meas, axis=1)\n",
    "\n",
    "    perf_tbl_pivot = perf_tbl_filtered.pivot(index='grp', columns='site', values='meas_formatted').reset_index()\n",
    "    \n",
    "    file_name = os.path.join(result_path, 'table', 'subgroup_'+ aki_subgrp + '_' + meas_type + '.csv')\n",
    "    perf_tbl_pivot.to_csv(file_name)\n",
    "    return perf_tbl_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb9553-3dec-4362-af67-5cdb6145bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance tables for subgroup analysis\n",
    "# AKI 1 at onset\n",
    "create_subgrp_table(perf_aki1_dict['cb'], 'aki1', 'rvsl', 'auroc')\n",
    "create_subgrp_table(perf_aki1_dict['cb'], 'aki1', 'stgup', 'auroc')\n",
    "create_subgrp_table(perf_aki1_dict['cb'], 'aki1', 'rvsl', 'auprc')\n",
    "create_subgrp_table(perf_aki1_dict['cb'], 'aki1', 'stgup', 'auprc')\n",
    "# AKI 2&3 at onset\n",
    "create_subgrp_table(perf_aki23_dict['cb'], 'aki_23', 'rvsl', 'auroc')\n",
    "create_subgrp_table(perf_aki23_dict['cb'], 'aki_23', 'stgup', 'auroc')\n",
    "create_subgrp_table(perf_aki23_dict['cb'], 'aki_23', 'rvsl', 'auprc')\n",
    "create_subgrp_table(perf_aki23_dict['cb'], 'aki_23', 'stgup', 'auprc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936897e1-0e30-4bc7-a43a-32b8e1870b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:37:41.318338Z",
     "start_time": "2023-12-03T23:37:39.324512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "import catboost as cb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Utility functions: Load & prepare data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7db1180c587fa688"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0aca00-bf12-4662-b87e-d256e2e97c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_discrete_survival_data_for_site(base_path, site_name, pred_pt, pred_task, fs_type, aki_subgrp):\n",
    "    dat_lst = []\n",
    "    pred_end = 7\n",
    "\n",
    "    filepath = os.path.join(base_path, site_name, 'processed_data') + '/'\n",
    "    outcome0 = pd.read_pickle(filepath + 'outcome.pkl')\n",
    "    demo = pd.read_pickle(base_path + site_name + '/AKI_DEMO' + '.pkl')\n",
    "    demo_deduplicated = demo[['PATID', 'ENCOUNTERID', 'DEATH_DATE']].drop_duplicates()\n",
    "    demo_cleaned = (demo_deduplicated\n",
    "                    .groupby(['PATID'], as_index=False)\n",
    "                    .agg({'DEATH_DATE': lambda x: x.max() if x.notna().any() else pd.NaT}))\n",
    "\n",
    "    outcome = outcome0.merge(demo_cleaned[['PATID', 'DEATH_DATE']],\n",
    "                             on='PATID',\n",
    "                             how='left')\n",
    "    outcome['DEATH_SINCE_ONSET'] = (outcome['DEATH_DATE'] - outcome['ONSET_DATE']).dt.days\n",
    "    outcome['DEATH_SINCE_ONSET'] = outcome['DEATH_SINCE_ONSET'].fillna(1000000)\n",
    "    outcome['DEATH_SINCE_ONSET'] = outcome['DEATH_SINCE_ONSET'].astype(int)\n",
    "    outcome.loc[outcome['DEATH_SINCE_ONSET'] < 0, 'DEATH_SINCE_ONSET'] = 1000000\n",
    "    outcome['DEATH_DISCHARGE_SINCE_ONSET'] = outcome[['DISCHARGE_SINCE_ONSET', 'DEATH_SINCE_ONSET']].min(axis=1)\n",
    "\n",
    "    outcome['AKI2_SINCE_ONSET'] = outcome['AKI2_SINCE_ADMIT'] - outcome['ONSET_SINCE_ADMIT']\n",
    "    outcome['AKI3_SINCE_ONSET'] = outcome['AKI3_SINCE_ADMIT'] - outcome['ONSET_SINCE_ADMIT']\n",
    "\n",
    "    if pred_pt < 7:\n",
    "        tw = list(range(pred_pt + 1, pred_end + 1)) if pred_task != 'rvsl' else list(range(pred_pt + 2, pred_end + 1))\n",
    "    else:\n",
    "        raise ValueError(f\"Prediction point pred_pt should be less than {pred_end}.\")\n",
    "\n",
    "    for t in tw:\n",
    "        if pred_task == 'rvsl':\n",
    "            tmp_out = \\\n",
    "            outcome[(~((outcome['RVRT_SINCE_ONSET'] + 1) < t)) & (outcome['DEATH_DISCHARGE_SINCE_ONSET'] >= t)][\n",
    "                ['PATID', 'ENCOUNTERID', 'RVRT_SINCE_ONSET']]\n",
    "            tmp_out['AKI_RVRT'] = ((tmp_out['RVRT_SINCE_ONSET'] + 1) <= t)\n",
    "            tmp_out.drop('RVRT_SINCE_ONSET', axis=1, inplace=True)\n",
    "        elif pred_task == 'stgup':\n",
    "            mask_stg1up = ((outcome['AKI_INIT_STG'] == 1) &\n",
    "                           ((outcome['AKI2_SINCE_ONSET'] >= t) | np.isnan(outcome['AKI2_SINCE_ONSET'])) &\n",
    "                           ((outcome['AKI3_SINCE_ONSET'] >= t) | np.isnan(outcome['AKI3_SINCE_ONSET'])))\n",
    "\n",
    "            mask_stg2up = ((outcome['AKI_INIT_STG'] == 2) &\n",
    "                           ((outcome['AKI3_SINCE_ONSET'] >= t) | np.isnan(outcome['AKI3_SINCE_ONSET'])))\n",
    "\n",
    "            tmp_out = outcome[(mask_stg1up | mask_stg2up) & (outcome['DEATH_DISCHARGE_SINCE_ONSET'] >= t)][\n",
    "                ['PATID', 'ENCOUNTERID',\n",
    "                 'AKI_INIT_STG',\n",
    "                 'AKI2_SINCE_ONSET',\n",
    "                 'AKI3_SINCE_ONSET']]\n",
    "            tmp_out['AKI_STGUP'] = (((tmp_out['AKI_INIT_STG'] == 1) & (\n",
    "                        (tmp_out['AKI2_SINCE_ONSET'] <= t) | (tmp_out['AKI3_SINCE_ONSET'] <= t))) |\n",
    "                                    ((tmp_out['AKI_INIT_STG'] == 2) & (tmp_out['AKI3_SINCE_ONSET'] <= t)))\n",
    "\n",
    "            tmp_out.drop(['AKI_INIT_STG', 'AKI2_SINCE_ONSET', 'AKI3_SINCE_ONSET'], axis=1, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pred_task: {pred_task}\")\n",
    "        \n",
    "        for s in tw:\n",
    "            tmp_out['POD_' + str(s)] = False\n",
    "\n",
    "        tmp_out['POD_' + str(t)] = True\n",
    "        tmp_out['ID_PAT_ENC'] = tmp_out['PATID'].astype(str) + '_' + tmp_out['ENCOUNTERID'].astype(str)\n",
    "        tmp_out['ID_POD'] = tmp_out['ID_PAT_ENC'] + '_' + str(t)\n",
    "\n",
    "        # load features for day t\n",
    "        t_dat = t - pred_pt if pred_task != 'rvsl' else (t - 1 - pred_pt)\n",
    "        data = pd.read_pickle(filepath + 'data_' + 'd' + str(t_dat) + '.pkl')\n",
    "        cols = ['ADMIT_DATE', 'DISCHARGE_DATE', 'ONSET_DATE', 'AKI1_SINCE_ADMIT', 'AKI2_SINCE_ADMIT',\n",
    "                'AKI3_SINCE_ADMIT',\n",
    "                'DISCHARGE_SINCE_ONSET', 'SCR_ONSET', 'SCR_REFERENCE', 'AKI1_7D', 'AKI1_2D', 'FLAG', 'AKI_STAGE',\n",
    "                'SCR_RANGE',\n",
    "                'SYSTOLIC_RANGE', 'WT', 'PREADM_CKD_STAGE', 'DIASTOLIC_RANGE', 'LAB_LG50024-5', 'LAB_LG6657-3']\n",
    "\n",
    "        cols_to_drop = [var for var in data.columns if var in cols]\n",
    "        data = data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "        # merge outcome and features\n",
    "        dat_t = tmp_out.merge(data, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "        if aki_subgrp == 'aki1':\n",
    "            dat_t = dat_t[dat_t['AKI_INIT_STG'] == 1].drop('AKI_INIT_STG', axis=1)\n",
    "        elif (aki_subgrp == 'aki23') & (pred_task == 'rvsl'):\n",
    "            dat_t = dat_t[dat_t['AKI_INIT_STG'] > 1].drop('AKI_INIT_STG', axis=1)\n",
    "        elif (aki_subgrp == 'aki23') & (pred_task == 'stgup'):\n",
    "            dat_t = dat_t[dat_t['AKI_INIT_STG'] == 2].drop('AKI_INIT_STG', axis=1)\n",
    "        dat_lst.append(dat_t)\n",
    "\n",
    "    all_columns = set(dat_lst[0].columns)\n",
    "    column_dtypes = {}\n",
    "\n",
    "    for col in dat_lst[-1].columns:\n",
    "        column_dtypes[col] = dat_lst[-1][col].dtype\n",
    "\n",
    "    for df in dat_lst[0:-1]:\n",
    "        all_columns.update(df.columns)\n",
    "        for col in df.columns:\n",
    "            if col not in column_dtypes:\n",
    "                column_dtypes[col] = df[col].dtype\n",
    "\n",
    "    all_columns = sorted(list(all_columns))\n",
    "\n",
    "    aligned_dat_lst = []\n",
    "    for df in dat_lst:\n",
    "        aligned_df = df.reindex(columns=all_columns)\n",
    "        aligned_dat_lst.append(aligned_df)\n",
    "\n",
    "    concatenated_df = pd.concat(aligned_dat_lst, axis=0, ignore_index=True)\n",
    "\n",
    "    for col in concatenated_df.columns:\n",
    "        concatenated_df[col] = concatenated_df[col].astype(column_dtypes[col])\n",
    "\n",
    "    bool_columns = concatenated_df.select_dtypes(include=['bool']).columns\n",
    "    concatenated_df[bool_columns] = concatenated_df[bool_columns].fillna(False)\n",
    "\n",
    "    if fs_type == 'rm_scr_bun':\n",
    "        scr_bun_labs = ['2160-0', '38483-4', '14682-9', '21232-4', '35203-9', '44784-7', '59826-8',\n",
    "                        '16188-5', '16189-3', '59826-8', '35591-7', '50380-5', '50381-3', '35592-5',\n",
    "                        '44784-7', '11041-1', '51620-3', '72271-0', '11042-9', '51619-5', '35203-9', '14682-9',\n",
    "                        '12966-8', '12965-0', '6299-2', '59570-2', '12964-3', '49071-4', '72270-2',\n",
    "                        '11065-0', '3094-0', '35234-4', '14937-7',\n",
    "                        '48642-3', '48643-1',  # eGFR\n",
    "                        '3097-3', '44734-2',  # scr bun ratio\n",
    "                        '12967-6', '13506-1', '20624-3', '2890-2', '33914-3', '34366-5', '62238-1', '88293-6',\n",
    "                        '88294-4',  # additional keys from athena\n",
    "                        'LG12083-8', 'LG1314-6', 'LG34710-0', 'LG34791-0', 'LG34808-2', 'LG35227-4', 'LG35814-9',\n",
    "                        'LG49763-2',  # loinc group id\n",
    "                        'LG49764-0', 'LG49776-4', 'LG50019-5', 'LG50024-5', 'LG50025-2', 'LG50986-5', 'LG6657-3',\n",
    "                        'LG7133-4']\n",
    "\n",
    "        scr_bun_cols = ['LAB_' + var for var in scr_bun_labs] + ['SCR_BASELINE', 'SCR_MEAN', 'SCR_FD', 'SCR_REFERENCE',\n",
    "                                                                 'SCR_RANGE', 'SCR_ONSET']\n",
    "\n",
    "        rm_cols = [var for var in concatenated_df.columns if var in scr_bun_cols]\n",
    "        concatenated_df.drop(rm_cols, axis=1, inplace=True)\n",
    "\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625c63b-5471-4743-877a-987c79c33a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catRandomSearch(data, outcome_label, validation_type, test_size,  n_folds = 10, n_iters = 50, rnd_split_seed = 42):\n",
    "    # a global seed for reproducibility\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    if validation_type == 'rnd':\n",
    "        enc_tmp = data['ID_PAT_ENC'].unique()\n",
    "        enc_tr_tmp, enc_ts = train_test_split(enc_tmp, test_size=test_size, random_state=rnd_split_seed)\n",
    "        enc_tr, enc_val = train_test_split(enc_tr_tmp, test_size=test_size, random_state=123)\n",
    "        data_train = data[data['ID_PAT_ENC'].isin(enc_tr)].reset_index(drop=True)\n",
    "        data_val = data[data['ID_PAT_ENC'].isin(enc_val)].reset_index(drop=True)\n",
    "        data_test = data[data['ID_PAT_ENC'].isin(enc_ts)].reset_index(drop=True)\n",
    "    elif validation_type == 'tmpr':\n",
    "        data_before_covid = data[data['BCCOVID'] == 1]\n",
    "        data_after_covid = data[data['BCCOVID'] == 0]\n",
    "        enc_before = data_before_covid['ID_PAT_ENC'].unique()\n",
    "        enc_tr, enc_val = train_test_split(enc_before, test_size=test_size, random_state=rnd_split_seed)\n",
    "        data_train = data_before_covid[data_before_covid['ID_PAT_ENC'].isin(enc_tr)].reset_index(drop=True)\n",
    "        data_val = data_before_covid[data_before_covid['ID_PAT_ENC'].isin(enc_val)].reset_index(drop=True)\n",
    "        data_test = data_after_covid.reset_index(drop=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown validation type: {validation_type}\")\n",
    "    \n",
    "    x_train = data_train.drop(['ID_POD', 'ID_PAT_ENC', 'PATID', 'ENCOUNTERID', outcome_label], axis=1)\n",
    "    y_train = data_train[outcome_label]\n",
    "\n",
    "    x_test = data_test.drop(['ID_POD', 'ID_PAT_ENC', 'PATID', 'ENCOUNTERID', outcome_label], axis=1)\n",
    "    y_test = data_test[outcome_label]\n",
    "\n",
    "    x_val = data_val.drop(['ID_POD', 'ID_PAT_ENC', 'PATID', 'ENCOUNTERID', outcome_label], axis=1)\n",
    "    y_val = data_val[outcome_label]\n",
    "\n",
    "    np.random.shuffle(enc_tr)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    folds = []\n",
    "    for train_idx, val_idx in kf.split(enc_tr):\n",
    "        train_enc_ids = enc_tr[train_idx]\n",
    "        test_enc_ids = enc_tr[val_idx]\n",
    "\n",
    "        # Get indices of rows for these encounters\n",
    "        train_indices = data_train.index[data_train['ID_PAT_ENC'].isin(train_enc_ids)].tolist()\n",
    "        val_indices = data_train.index[data_train['ID_PAT_ENC'].isin(test_enc_ids)].tolist()\n",
    "\n",
    "        folds.append([train_indices, val_indices])\n",
    "\n",
    "    labelcount = y_train.value_counts()\n",
    "    cat_features = list(x_train.select_dtypes(include=['bool']).columns)\n",
    "    cvmodel = cb.CatBoostClassifier(\n",
    "        scale_pos_weight=labelcount[0] / labelcount[1],\n",
    "        objective='Logloss',\n",
    "        eval_metric='AUC:hints=skip_train~false',\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=500,\n",
    "        cat_features=cat_features,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        'subsample': [0.5, 0.8, 1.0],\n",
    "        'colsample_bylevel': [0.5, 0.8, 1.0],\n",
    "        'max_depth': [6, 8, 12],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 400],\n",
    "        'l2_leaf_reg': [3, 5],\n",
    "        'random_strength': [1, 3, 5],\n",
    "        'bagging_temperature': [0.1, 0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    randomized_search_result = cvmodel.randomized_search(\n",
    "        params,\n",
    "        X=x_train,\n",
    "        y=y_train,\n",
    "        cv=folds,\n",
    "        n_iter=n_iters\n",
    "    )\n",
    "\n",
    "    bestmodel = cb.CatBoostClassifier(\n",
    "        scale_pos_weight=labelcount[0] / labelcount[1],\n",
    "        objective='Logloss',\n",
    "        eval_metric='AUC:hints=skip_train~false',\n",
    "        verbose=250, # type: ignore[arg-type]\n",
    "        early_stopping_rounds=500,\n",
    "        cat_features=cat_features,\n",
    "        random_seed=42,\n",
    "        **randomized_search_result['params']\n",
    "    )\n",
    "\n",
    "    bestmodel.fit(x_train, y_train,\n",
    "                  cat_features=cat_features,\n",
    "                  eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "                  early_stopping_rounds=500\n",
    "                  )\n",
    "\n",
    "    y_pred = bestmodel.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    return {\n",
    "        'best_model': bestmodel,\n",
    "        'data_test': data_test,\n",
    "        'data_val': data_val,\n",
    "        'y_pred': y_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c2a9a-ba95-467a-bf5f-110796138ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRandomSearch(data, outcome_label, validation_type,  test_size, n_folds = 10, n_iters = 50, rnd_split_seed = 42):\n",
    "    # a global seed for reproducibility\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    if validation_type == 'rnd':\n",
    "        enc_tmp = data['ID_PAT_ENC'].unique()\n",
    "        enc_tr_tmp, enc_ts = train_test_split(enc_tmp, test_size=test_size, random_state=rnd_split_seed)\n",
    "        enc_tr, enc_val = train_test_split(enc_tr_tmp, test_size=test_size, random_state=123)\n",
    "        data_train = data[data['ID_PAT_ENC'].isin(enc_tr)].reset_index(drop=True)\n",
    "        data_val = data[data['ID_PAT_ENC'].isin(enc_val)].reset_index(drop=True)\n",
    "        data_test = data[data['ID_PAT_ENC'].isin(enc_ts)].reset_index(drop=True)\n",
    "    elif validation_type == 'tmpr':\n",
    "        data_before_covid = data[data['BCCOVID'] == 1]\n",
    "        data_after_covid = data[data['BCCOVID'] == 0]\n",
    "        enc_before = data_before_covid['ID_PAT_ENC'].unique()\n",
    "        enc_tr, enc_val = train_test_split(enc_before, test_size=test_size, random_state=rnd_split_seed)\n",
    "        data_train = data_before_covid[data_before_covid['ID_PAT_ENC'].isin(enc_tr)].reset_index(drop=True)\n",
    "        data_val = data_before_covid[data_before_covid['ID_PAT_ENC'].isin(enc_val)].reset_index(drop=True)\n",
    "        data_test = data_after_covid.reset_index(drop=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown validation type: {validation_type}\")\n",
    "    \n",
    "    data_test_raw = data_test.copy()\n",
    "    x_train = data_train.drop(['ID_POD', 'ID_PAT_ENC', 'PATID', 'ENCOUNTERID', 'BCCOVID', outcome_label], axis=1)\n",
    "    y_train = data_train[outcome_label]\n",
    "\n",
    "    x_test = data_test.drop(['ID_POD', 'ID_PAT_ENC', 'PATID', 'ENCOUNTERID', 'BCCOVID', outcome_label], axis=1)\n",
    "    y_test = data_test[outcome_label]\n",
    "\n",
    "    preprocessor = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    x_train_processed = pd.DataFrame(preprocessor.fit_transform(x_train), columns=x_train.columns)\n",
    "\n",
    "    model = LogisticRegression(penalty='l1',\n",
    "                               solver='liblinear',\n",
    "                               max_iter=1000)\n",
    "\n",
    "    param_distributions = {\n",
    "        'C': np.logspace(-3, 1, 20)\n",
    "    }\n",
    "\n",
    "    np.random.shuffle(enc_tr)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "    folds = []\n",
    "    for train_idx, val_idx in kf.split(enc_tr):\n",
    "        train_enc_ids = enc_tr[train_idx]\n",
    "        test_enc_ids = enc_tr[val_idx]\n",
    "\n",
    "        train_indices = data_train.index[data_train['ID_PAT_ENC'].isin(train_enc_ids)].tolist()\n",
    "        val_indices = data_train.index[data_train['ID_PAT_ENC'].isin(test_enc_ids)].tolist()\n",
    "\n",
    "        folds.append([train_indices, val_indices])\n",
    "\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions,\n",
    "        n_iter=n_iters,\n",
    "        cv=kf,\n",
    "        random_state=rnd_split_seed,\n",
    "        scoring='roc_auc',\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    randomized_search.fit(x_train_processed, y_train)\n",
    "\n",
    "    bestmodel = randomized_search.best_estimator_\n",
    "\n",
    "    x_test_processed = pd.DataFrame(preprocessor.transform(x_test), columns=x_test.columns)\n",
    "    y_pred = randomized_search.predict_proba(x_test_processed)[:, 1]\n",
    "\n",
    "    data_test_imputed_scaled = x_test_processed\n",
    "    data_test_imputed_scaled[outcome_label] = y_test.reset_index(drop=True)\n",
    "\n",
    "    return {\n",
    "        'best_model': bestmodel,\n",
    "        'data_test_raw': data_test_raw,\n",
    "        'data_test': data_test_imputed_scaled,\n",
    "        'data_val': data_val,\n",
    "        'y_pred': y_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Runtime: Train predictive models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43b97617fe36bf05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1af8c9-6833-499c-9c5f-181debb445cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./\"\n",
    "model_path = os.path.join(base_path, \"model\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "site_labels = ['Site1', 'Site2', 'Site3', 'Site4']\n",
    "site_mapping = {\n",
    "    \"... MASKED_FOR_ANONYMITY\": \"... MASKED_FOR_ANONYMITY\",\n",
    "    # ...\n",
    "}\n",
    "\n",
    "test_size = 0.15\n",
    "prediction_tasks = [\"rvsl\", \"stgup\"]\n",
    "model_types = [\"cb\", \"rlr\"]\n",
    "validation_types = [\"rnd\", \"tmpr\"]\n",
    "aki_subgroups = [\"aki1\", \"aki23\"]\n",
    "fs_types = [\"no_fs\", \"rm_scr_bun\"]\n",
    "\n",
    "for model_type in model_types:\n",
    "    for val_type in validation_types:\n",
    "        for aki_subgrp in aki_subgroups:\n",
    "            for fs_type in fs_types:\n",
    "                result_dict = {}\n",
    "\n",
    "                for pred_task in prediction_tasks:\n",
    "                    if pred_task == \"rvsl\":\n",
    "                        outcome_label = \"AKI_RVRT\"\n",
    "                    elif pred_task == \"stgup\":\n",
    "                        outcome_label = \"AKI_STGUP\"\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown pred_task: {pred_task}\")\n",
    "\n",
    "                    result_dict[pred_task] = {}\n",
    "\n",
    "                    for site_label in site_labels:\n",
    "                        try:\n",
    "                            start_time = time.time()\n",
    "\n",
    "                            # Get model data\n",
    "                            model_data = load_discrete_survival_data_for_site(\n",
    "                                base_path=base_path,\n",
    "                                site_name=site_mapping.get(site_label, site_label),\n",
    "                                pred_pt=0,\n",
    "                                pred_task=pred_task,\n",
    "                                fs_type=fs_type,\n",
    "                                aki_subgrp=aki_subgrp\n",
    "                            )\n",
    "\n",
    "                            # Model fitting\n",
    "                            if model_type == \"cb\":\n",
    "                                site_result = catRandomSearch(\n",
    "                                    model_data,\n",
    "                                    outcome_label,\n",
    "                                    val_type,\n",
    "                                    test_size\n",
    "                                )\n",
    "                            elif model_type == \"rlr\":\n",
    "                                site_result = logisticRandomSearch(\n",
    "                                    model_data,\n",
    "                                    outcome_label,\n",
    "                                    val_type,\n",
    "                                    test_size\n",
    "                                )\n",
    "                            else:\n",
    "                                raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "\n",
    "                            elapsed_time = time.time() - start_time\n",
    "                            site_result[\"elapsed_time\"] = elapsed_time\n",
    "\n",
    "                            result_dict[pred_task][site_label] = site_result\n",
    "\n",
    "                            # Create output filename\n",
    "                            if val_type == \"rnd\":\n",
    "                                file_name = f\"{model_type}_{aki_subgrp}_{pred_task}_1d_{fs_type}.pkl\"\n",
    "                            elif val_type == \"tmpr\": \n",
    "                                file_name = f\"{model_type}_tmpr_{aki_subgrp}_{pred_task}_1d_{fs_type}.pkl\"\n",
    "                            else:\n",
    "                                raise ValueError(f\"Unknown validation type: {val_type}\")\n",
    "\n",
    "                            file_path = os.path.join(model_path, file_name)\n",
    "                            with open(file_path, \"wb\") as file:\n",
    "                                pickle.dump(result_dict[pred_task], file)\n",
    "\n",
    "                            print(\n",
    "                                f\"Completed: {model_type}, {val_type}, {aki_subgrp}, {pred_task}, {fs_type} for site: {site_label}\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            err_msg = (f\"Error running model={model_type}, val={val_type}, fs={fs_type}, \"\n",
    "                                       f\"site={site_label}, task={pred_task}: {str(e)}\")\n",
    "                            logging.error(err_msg)\n",
    "                            print(err_msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
